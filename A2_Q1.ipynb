{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUgt3Yub/ukfNxtn27av8V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammed-Taasir/CSE508_Winter2023_A2_14/blob/main/A2_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K6yWjgMRM3YV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c379a7-2491-4fc4-8db4-4aca6a3252a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/MyDrive/IR Assignment-1/CSE508_Winter2023_Dataset.zip' '/content/'\n",
        "!unzip 'CSE508_Winter2023_Dataset.zip' &> /dev/null\n",
        "!rm 'CSE508_Winter2023_Dataset.zip'"
      ],
      "metadata": {
        "id": "liUhbCY4RzoP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/CSE508_Winter2023_Dataset/'"
      ],
      "metadata": {
        "id": "vqIaHYCjSJfS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "import random\n",
        "import math\n",
        "import heapq\n",
        "import operator\n",
        "from collections import Counter\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "qqARjmOqTB7a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sortedcontainers import SortedDict, SortedList, SortedSet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import PlaintextCorpusReader \n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "9hMpK8l_TWXP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Vjwl0_ZTYZP",
        "outputId": "faf6c417-7b12-4755-d391-2f8a87a4fc97"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Helper Functions"
      ],
      "metadata": {
        "id": "XKJzHpviTlJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read from files"
      ],
      "metadata": {
        "id": "4_Bv4U1ETn0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  def getListOfFiles(directory):\n",
        "\n",
        "    # Parameters: directory: type(string)        \n",
        "    # returns: list of all files in directory with the full path of file\n",
        "    \n",
        "    list_of_files = []\n",
        "\n",
        "    for i in range(len(filenames)):\n",
        "      fi = \"/content/CSE508_Winter2023_Dataset/\"+filenames[i]\n",
        "      list_of_files.append(fi)\n",
        "    \n",
        "    return list_of_files"
      ],
      "metadata": {
        "id": "PmWxb84lTZtg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Functions"
      ],
      "metadata": {
        "id": "cFd_lFuZTvkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase(data):\n",
        "\n",
        "    # Parameters: data: type(string)\n",
        "    # returns: lowercase of data   \n",
        "     \n",
        "    return data.lower()"
      ],
      "metadata": {
        "id": "30q_9cXETrrr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_word_tokenize(corpus):\n",
        "  \n",
        "    # Parameters:corpus: type(string)   \n",
        "    # returns word-level tokenization of corpus\n",
        "\n",
        "    return word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "Xbku60vtVIyw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords_from_tokens(tokens, stopwords_set):\n",
        "  \n",
        "    # Parameters: tokens: type(list)\n",
        "    #             stopwords_set: type(set)\n",
        "    # returns: tokens without stopwords\n",
        "\n",
        "    tokens_sans_stopwords = [x for x in tokens if x not in stopwords_set] \n",
        "    return tokens_sans_stopwords"
      ],
      "metadata": {
        "id": "wv8KEcziTxQC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation_from_tokens(tokens):\n",
        "\n",
        "    # Parameters: tokens: type(list)\n",
        "    # returns: tokens without punctuation\n",
        "\n",
        "    tokens_sans_punctuation = [x.translate(str.maketrans('', '', string.punctuation)) for x in tokens]\n",
        "    return tokens_sans_punctuation"
      ],
      "metadata": {
        "id": "aWoN_okWT-IL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_blank_space_tokens(tokens):\n",
        "    \n",
        "    #Parameters: tokens: type(list)\n",
        "    #returns: tokens without blank tokens\n",
        "\n",
        "    tokens_sans_blank_space = [x for x in tokens if x!='']  \n",
        "    return tokens_sans_blank_space"
      ],
      "metadata": {
        "id": "im3CrcTeUHGE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus, stopwords_set):\n",
        "    # Convert the text to lower case\n",
        "    lowercase_corpus = lowercase(corpus)\n",
        "    #print(len(lowercase_corpus))\n",
        "    \n",
        "    # Perform word tokenization (word_tokenize also takes care of whitespace)\n",
        "    word_tokens = perform_word_tokenize(lowercase_corpus)\n",
        "    #print(len(word_tokens))\n",
        "    \n",
        "    # Remove stopwords from tokens\n",
        "    word_tokens_sans_stopwords = remove_stopwords_from_tokens(word_tokens, stopwords_set)\n",
        "    #print(len(word_tokens_sans_stopwords))\n",
        "    \n",
        "    # Remove punctuation marks from tokens\n",
        "    word_tokens_sans_punctuation = remove_punctuation_from_tokens(word_tokens_sans_stopwords)\n",
        "    #print(len(word_tokens_sans_punctuation))\n",
        "    \n",
        "    # Remove blank space tokens\n",
        "    word_tokens_sans_blank_tokens = remove_blank_space_tokens(word_tokens_sans_punctuation)\n",
        "    #print(len(word_tokens_sans_blank_tokens))\n",
        "    \n",
        "    # Stem tokens\n",
        "    #word_tokens_final = stemming(word_tokens_sans_blank_tokens)\n",
        "    \n",
        "    return word_tokens_sans_blank_tokens"
      ],
      "metadata": {
        "id": "XVFFgAnZUJEQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_documents(list_of_files, stopwords_set):\n",
        "    '''\n",
        "    Parameters:\n",
        "        list_of_files: type(list)\n",
        "        stopwords_set: type(set)\n",
        "    \n",
        "    returns: list of tokens obtained by preprocessing documents in all classes\n",
        "    '''\n",
        "    preprocessed_list_of_docs_tokens = []\n",
        "    for doc_path in list_of_files:\n",
        "        file = open(doc_path, 'r', encoding='utf-8', errors='ignore')\n",
        "        file_corpus = file.read()\n",
        "        file.close()\n",
        "        doc_tokens = preprocess(file_corpus, stopwords_set)\n",
        "        preprocessed_list_of_docs_tokens.append(doc_tokens)\n",
        "    \n",
        "    pi_file = open('Q1_tf_idf.pkl', 'wb')\n",
        "    pickle.dump(preprocessed_list_of_docs_tokens, pi_file)\n",
        "    pi_file.close()\n",
        "    \n",
        "    return preprocessed_list_of_docs_tokens"
      ],
      "metadata": {
        "id": "t3O2ET20cKOk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_file_dictionary(list_of_files):\n",
        "    \n",
        "    # Paramteres: list_of_files: type(string)\n",
        "    # returns: file_dictionary with integer key and path_of_file as value\n",
        "  \n",
        "    file_dictionary = {}\n",
        "    for i in range(len(list_of_files)):\n",
        "        file_dictionary[i] = list_of_files[i]\n",
        "    \n",
        "    return file_dictionary"
      ],
      "metadata": {
        "id": "d3qCJQZVUMLA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. Data Preprocessing"
      ],
      "metadata": {
        "id": "SvAA72ftUTzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (i) Relevant Text Extraction"
      ],
      "metadata": {
        "id": "PUwrbpCMUXS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_root='/content/CSE508_Winter2023_Dataset/'\n",
        "corpus=PlaintextCorpusReader(corpus_root,'.*')"
      ],
      "metadata": {
        "id": "Hu4lZqDwURJ7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing names of all files\n",
        "\n",
        "filenames=corpus.fileids()\n",
        "# print(filenames)"
      ],
      "metadata": {
        "id": "g0hsNxV1UZ3L"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(filenames))\n",
        "print(len(filenames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH1WlCchUb76",
        "outputId": "3b6223a5-730b-4595-f55a-79fed033b6bf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "1400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First 5 Documents"
      ],
      "metadata": {
        "id": "pby700qfUgHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing contents of first 5 files before extracting\n",
        "\n",
        "for i in range(0, 5):\n",
        "  outfile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "  doc = outfile.read()\n",
        "  print(\"------------\")\n",
        "  print(\"Document: \"+str(i+1))\n",
        "  print(\"------------\")\n",
        "  print(doc)\n",
        "  print('-----------------------------------------------------------------')\n",
        "  print('')\n",
        "  outfile.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkLMEk3FUeCG",
        "outputId": "94d9095a-fad9-46ab-f756-6e13b047d6b4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------\n",
            "Document: 1\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "1\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "experimental investigation of the aerodynamics of a\n",
            "wing in a slipstream .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "brenckman,m.\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "j. ae. scs. 25, 1958, 324.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "  an experimental study of a wing in a propeller slipstream was\n",
            "made in order to determine the spanwise distribution of the lift\n",
            "increase due to slipstream at different angles of attack of the wing\n",
            "and at different free stream to slipstream velocity ratios .  the\n",
            "results were intended in part as an evaluation basis for different\n",
            "theoretical treatments of this problem .\n",
            "  the comparative span loading curves, together with supporting\n",
            "evidence, showed that a substantial part of the lift increment\n",
            "produced by the slipstream was due to a /destalling/ or boundary-layer-control\n",
            "effect .  the integrated remaining lift increment,\n",
            "after subtracting this destalling lift, was found to agree\n",
            "well with a potential flow theory .\n",
            "  an empirical evaluation of the destalling effects was made for\n",
            "the specific configuration of the experiment .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 2\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "2\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "simple shear flow past a flat plate in an incompressible fluid of small\n",
            "viscosity .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "ting-yili\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "department of aeronautical engineering, rensselaer polytechnic\n",
            "institute\n",
            "troy, n.y.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "in the study of high-speed viscous flow past a two-dimensional body it\n",
            "is usually necessary to consider a curved shock wave emitting from the\n",
            "nose or leading edge of the body .  consequently, there exists an inviscid\n",
            "rotational flow region between the shock wave and the boundary layer\n",
            ".  such a situation arises, for instance, in the study of the hypersonic\n",
            "viscous flow past a flat plate .  the situation is somewhat different\n",
            "from prandtl's classical boundary-layer problem . in prandtl's\n",
            "original problem the inviscid free stream outside the boundary layer is\n",
            "irrotational while in a hypersonic boundary-layer problem the inviscid\n",
            "free stream must be considered as rotational .  the possible effects of\n",
            "vorticity have been recently discussed by ferri and libby .  in the present\n",
            "paper, the simple shear flow past a flat plate in a fluid of small\n",
            "viscosity is investigated .  it can be shown that this problem can again\n",
            "be treated by the boundary-layer approximation, the only novel feature\n",
            "being that the free stream has a constant vorticity .  the discussion\n",
            "here is restricted to two-dimensional incompressible steady flow .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 3\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "3\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "the boundary layer in simple shear flow past a flat plate .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "m. b. glauert\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "department of mathematics, university of manchester, manchester,\n",
            "england\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "the boundary-layer equations are presented for steady\n",
            "incompressible flow with no pressure gradient .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 4\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "4\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "approximate solutions of the incompressible laminar\n",
            "boundary layer equations for a plate in shear flow .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "yen,k.t.\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "j. ae. scs. 22, 1955, 728.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "  the two-dimensional steady boundary-layer\n",
            "problem for a flat plate in a\n",
            "shear flow of incompressible fluid is considered .\n",
            "solutions for the boundarylayer\n",
            "thickness, skin friction, and the velocity\n",
            "distribution in the boundary\n",
            "layer are obtained by the karman-pohlhausen\n",
            "technique .  comparison with\n",
            "the boundary layer of a uniform flow has also\n",
            "been made to show the effect of\n",
            "vorticity .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 5\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "5\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "one-dimensional transient heat conduction into a double-layer\n",
            "slab subjected to a linear heat input for a small time\n",
            "internal .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "wasserman,b.\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "j. ae. scs. 24, 1957, 924.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "  analytic solutions are presented for the transient heat conduction\n",
            "in composite slabs exposed at one surface to a\n",
            "triangular heat rate .  this type of heating rate may occur, for\n",
            "example, during aerodynamic heating .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting content between TITLE and TEXT tag from all documents."
      ],
      "metadata": {
        "id": "yQxD-lsWUo-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(filenames)):\n",
        "    outfile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "    doc = outfile.read()\n",
        "\n",
        "    # initializing string\n",
        "    test_str = doc\n",
        "\n",
        "    # initializing tags\n",
        "    tag1 = \"TITLE\"\n",
        "    tag2 = \"TEXT\"\n",
        "\n",
        "    # regex to extract required strings\n",
        "    reg_str1 = \"<\"+tag1+\">(.*?)</\"+tag1+\">\"\n",
        "    res1 = re.findall(reg_str1, test_str, re.DOTALL)\n",
        "\n",
        "    reg_str2 = \"<\"+tag2+\">(.*?)</\"+tag2+\">\"\n",
        "    res2 = re.findall(reg_str2, test_str, re.DOTALL)\n",
        "\n",
        "    #Combining contents of TITLE and TEXT\n",
        "    res = res1+res2\n",
        "    \n",
        "    s = res\n",
        "\n",
        "    # using list comprehension\n",
        "    listToStr = ' '.join([str(elem) for elem in s])\n",
        "\n",
        "    writeFile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'w')\n",
        "    L = listToStr\n",
        "\n",
        "    writeFile.write(L)\n",
        "    writeFile.close()\n",
        "\n",
        "    outfile.close()"
      ],
      "metadata": {
        "id": "Lm6oSqqjUiLo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First 5 Documents after extracting contents between TITLE and TEXT tag."
      ],
      "metadata": {
        "id": "WBuRQj9QUuvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 5):\n",
        "  outfile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "  doc = outfile.read()\n",
        "  print(\"------------\")\n",
        "  print(\"Document: \"+str(i+1))\n",
        "  print(\"------------\")\n",
        "  print(doc)\n",
        "  print('-----------------------------------------------------------------')\n",
        "  print('')\n",
        "  outfile.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjaG6ro6Ur2p",
        "outputId": "9ebe63e7-fef6-4c6f-b389-580e7434f824"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------\n",
            "Document: 1\n",
            "------------\n",
            "\n",
            "experimental investigation of the aerodynamics of a\n",
            "wing in a slipstream .\n",
            " \n",
            "  an experimental study of a wing in a propeller slipstream was\n",
            "made in order to determine the spanwise distribution of the lift\n",
            "increase due to slipstream at different angles of attack of the wing\n",
            "and at different free stream to slipstream velocity ratios .  the\n",
            "results were intended in part as an evaluation basis for different\n",
            "theoretical treatments of this problem .\n",
            "  the comparative span loading curves, together with supporting\n",
            "evidence, showed that a substantial part of the lift increment\n",
            "produced by the slipstream was due to a /destalling/ or boundary-layer-control\n",
            "effect .  the integrated remaining lift increment,\n",
            "after subtracting this destalling lift, was found to agree\n",
            "well with a potential flow theory .\n",
            "  an empirical evaluation of the destalling effects was made for\n",
            "the specific configuration of the experiment .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 2\n",
            "------------\n",
            "\n",
            "simple shear flow past a flat plate in an incompressible fluid of small\n",
            "viscosity .\n",
            " \n",
            "in the study of high-speed viscous flow past a two-dimensional body it\n",
            "is usually necessary to consider a curved shock wave emitting from the\n",
            "nose or leading edge of the body .  consequently, there exists an inviscid\n",
            "rotational flow region between the shock wave and the boundary layer\n",
            ".  such a situation arises, for instance, in the study of the hypersonic\n",
            "viscous flow past a flat plate .  the situation is somewhat different\n",
            "from prandtl's classical boundary-layer problem . in prandtl's\n",
            "original problem the inviscid free stream outside the boundary layer is\n",
            "irrotational while in a hypersonic boundary-layer problem the inviscid\n",
            "free stream must be considered as rotational .  the possible effects of\n",
            "vorticity have been recently discussed by ferri and libby .  in the present\n",
            "paper, the simple shear flow past a flat plate in a fluid of small\n",
            "viscosity is investigated .  it can be shown that this problem can again\n",
            "be treated by the boundary-layer approximation, the only novel feature\n",
            "being that the free stream has a constant vorticity .  the discussion\n",
            "here is restricted to two-dimensional incompressible steady flow .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 3\n",
            "------------\n",
            "\n",
            "the boundary layer in simple shear flow past a flat plate .\n",
            " \n",
            "the boundary-layer equations are presented for steady\n",
            "incompressible flow with no pressure gradient .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 4\n",
            "------------\n",
            "\n",
            "approximate solutions of the incompressible laminar\n",
            "boundary layer equations for a plate in shear flow .\n",
            " \n",
            "  the two-dimensional steady boundary-layer\n",
            "problem for a flat plate in a\n",
            "shear flow of incompressible fluid is considered .\n",
            "solutions for the boundarylayer\n",
            "thickness, skin friction, and the velocity\n",
            "distribution in the boundary\n",
            "layer are obtained by the karman-pohlhausen\n",
            "technique .  comparison with\n",
            "the boundary layer of a uniform flow has also\n",
            "been made to show the effect of\n",
            "vorticity .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 5\n",
            "------------\n",
            "\n",
            "one-dimensional transient heat conduction into a double-layer\n",
            "slab subjected to a linear heat input for a small time\n",
            "internal .\n",
            " \n",
            "  analytic solutions are presented for the transient heat conduction\n",
            "in composite slabs exposed at one surface to a\n",
            "triangular heat rate .  this type of heating rate may occur, for\n",
            "example, during aerodynamic heating .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (ii) Preprocessing"
      ],
      "metadata": {
        "id": "7nFfu7j0Uzib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Lowercase the text"
      ],
      "metadata": {
        "id": "GTETLsLRU1QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newfiles=[]\n",
        "for i in range(len(filenames)):\n",
        "  fileind=open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "  filedata=fileind.read()\n",
        "  filedata=lowercase(filedata)\n",
        "  newfiles.append(filedata)\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m74_zIXxUwP1",
        "outputId": "b0a5d260-e98a-44d9-edd1-e11de0ca031d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "\n",
            "experimental investigation of the aerodynamics of a\n",
            "wing in a slipstream .\n",
            " \n",
            "  an experimental study of a wing in a propeller slipstream was\n",
            "made in order to determine the spanwise distribution of the lift\n",
            "increase due to slipstream at different angles of attack of the wing\n",
            "and at different free stream to slipstream velocity ratios .  the\n",
            "results were intended in part as an evaluation basis for different\n",
            "theoretical treatments of this problem .\n",
            "  the comparative span loading curves, together with supporting\n",
            "evidence, showed that a substantial part of the lift increment\n",
            "produced by the slipstream was due to a /destalling/ or boundary-layer-control\n",
            "effect .  the integrated remaining lift increment,\n",
            "after subtracting this destalling lift, was found to agree\n",
            "well with a potential flow theory .\n",
            "  an empirical evaluation of the destalling effects was made for\n",
            "the specific configuration of the experiment .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "\n",
            "simple shear flow past a flat plate in an incompressible fluid of small\n",
            "viscosity .\n",
            " \n",
            "in the study of high-speed viscous flow past a two-dimensional body it\n",
            "is usually necessary to consider a curved shock wave emitting from the\n",
            "nose or leading edge of the body .  consequently, there exists an inviscid\n",
            "rotational flow region between the shock wave and the boundary layer\n",
            ".  such a situation arises, for instance, in the study of the hypersonic\n",
            "viscous flow past a flat plate .  the situation is somewhat different\n",
            "from prandtl's classical boundary-layer problem . in prandtl's\n",
            "original problem the inviscid free stream outside the boundary layer is\n",
            "irrotational while in a hypersonic boundary-layer problem the inviscid\n",
            "free stream must be considered as rotational .  the possible effects of\n",
            "vorticity have been recently discussed by ferri and libby .  in the present\n",
            "paper, the simple shear flow past a flat plate in a fluid of small\n",
            "viscosity is investigated .  it can be shown that this problem can again\n",
            "be treated by the boundary-layer approximation, the only novel feature\n",
            "being that the free stream has a constant vorticity .  the discussion\n",
            "here is restricted to two-dimensional incompressible steady flow .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "\n",
            "the boundary layer in simple shear flow past a flat plate .\n",
            " \n",
            "the boundary-layer equations are presented for steady\n",
            "incompressible flow with no pressure gradient .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "\n",
            "approximate solutions of the incompressible laminar\n",
            "boundary layer equations for a plate in shear flow .\n",
            " \n",
            "  the two-dimensional steady boundary-layer\n",
            "problem for a flat plate in a\n",
            "shear flow of incompressible fluid is considered .\n",
            "solutions for the boundarylayer\n",
            "thickness, skin friction, and the velocity\n",
            "distribution in the boundary\n",
            "layer are obtained by the karman-pohlhausen\n",
            "technique .  comparison with\n",
            "the boundary layer of a uniform flow has also\n",
            "been made to show the effect of\n",
            "vorticity .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "\n",
            "one-dimensional transient heat conduction into a double-layer\n",
            "slab subjected to a linear heat input for a small time\n",
            "internal .\n",
            " \n",
            "  analytic solutions are presented for the transient heat conduction\n",
            "in composite slabs exposed at one surface to a\n",
            "triangular heat rate .  this type of heating rate may occur, for\n",
            "example, during aerodynamic heating .\n",
            "\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Perform tokenization"
      ],
      "metadata": {
        "id": "Ayl0Ud2rU8RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  filedata=perform_word_tokenize(filedata)\n",
        "  newfiles[i] = filedata\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-gARkTaU5ZU",
        "outputId": "a7f18cd6-394c-491c-c1d3-6240fb638198"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'of', 'the', 'aerodynamics', 'of', 'a', 'wing', 'in', 'a', 'slipstream', '.', 'an', 'experimental', 'study', 'of', 'a', 'wing', 'in', 'a', 'propeller', 'slipstream', 'was', 'made', 'in', 'order', 'to', 'determine', 'the', 'spanwise', 'distribution', 'of', 'the', 'lift', 'increase', 'due', 'to', 'slipstream', 'at', 'different', 'angles', 'of', 'attack', 'of', 'the', 'wing', 'and', 'at', 'different', 'free', 'stream', 'to', 'slipstream', 'velocity', 'ratios', '.', 'the', 'results', 'were', 'intended', 'in', 'part', 'as', 'an', 'evaluation', 'basis', 'for', 'different', 'theoretical', 'treatments', 'of', 'this', 'problem', '.', 'the', 'comparative', 'span', 'loading', 'curves', ',', 'together', 'with', 'supporting', 'evidence', ',', 'showed', 'that', 'a', 'substantial', 'part', 'of', 'the', 'lift', 'increment', 'produced', 'by', 'the', 'slipstream', 'was', 'due', 'to', 'a', '/destalling/', 'or', 'boundary-layer-control', 'effect', '.', 'the', 'integrated', 'remaining', 'lift', 'increment', ',', 'after', 'subtracting', 'this', 'destalling', 'lift', ',', 'was', 'found', 'to', 'agree', 'well', 'with', 'a', 'potential', 'flow', 'theory', '.', 'an', 'empirical', 'evaluation', 'of', 'the', 'destalling', 'effects', 'was', 'made', 'for', 'the', 'specific', 'configuration', 'of', 'the', 'experiment', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'a', 'flat', 'plate', 'in', 'an', 'incompressible', 'fluid', 'of', 'small', 'viscosity', '.', 'in', 'the', 'study', 'of', 'high-speed', 'viscous', 'flow', 'past', 'a', 'two-dimensional', 'body', 'it', 'is', 'usually', 'necessary', 'to', 'consider', 'a', 'curved', 'shock', 'wave', 'emitting', 'from', 'the', 'nose', 'or', 'leading', 'edge', 'of', 'the', 'body', '.', 'consequently', ',', 'there', 'exists', 'an', 'inviscid', 'rotational', 'flow', 'region', 'between', 'the', 'shock', 'wave', 'and', 'the', 'boundary', 'layer', '.', 'such', 'a', 'situation', 'arises', ',', 'for', 'instance', ',', 'in', 'the', 'study', 'of', 'the', 'hypersonic', 'viscous', 'flow', 'past', 'a', 'flat', 'plate', '.', 'the', 'situation', 'is', 'somewhat', 'different', 'from', 'prandtl', \"'s\", 'classical', 'boundary-layer', 'problem', '.', 'in', \"prandtl's\", 'original', 'problem', 'the', 'inviscid', 'free', 'stream', 'outside', 'the', 'boundary', 'layer', 'is', 'irrotational', 'while', 'in', 'a', 'hypersonic', 'boundary-layer', 'problem', 'the', 'inviscid', 'free', 'stream', 'must', 'be', 'considered', 'as', 'rotational', '.', 'the', 'possible', 'effects', 'of', 'vorticity', 'have', 'been', 'recently', 'discussed', 'by', 'ferri', 'and', 'libby', '.', 'in', 'the', 'present', 'paper', ',', 'the', 'simple', 'shear', 'flow', 'past', 'a', 'flat', 'plate', 'in', 'a', 'fluid', 'of', 'small', 'viscosity', 'is', 'investigated', '.', 'it', 'can', 'be', 'shown', 'that', 'this', 'problem', 'can', 'again', 'be', 'treated', 'by', 'the', 'boundary-layer', 'approximation', ',', 'the', 'only', 'novel', 'feature', 'being', 'that', 'the', 'free', 'stream', 'has', 'a', 'constant', 'vorticity', '.', 'the', 'discussion', 'here', 'is', 'restricted', 'to', 'two-dimensional', 'incompressible', 'steady', 'flow', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['the', 'boundary', 'layer', 'in', 'simple', 'shear', 'flow', 'past', 'a', 'flat', 'plate', '.', 'the', 'boundary-layer', 'equations', 'are', 'presented', 'for', 'steady', 'incompressible', 'flow', 'with', 'no', 'pressure', 'gradient', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'of', 'the', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'for', 'a', 'plate', 'in', 'shear', 'flow', '.', 'the', 'two-dimensional', 'steady', 'boundary-layer', 'problem', 'for', 'a', 'flat', 'plate', 'in', 'a', 'shear', 'flow', 'of', 'incompressible', 'fluid', 'is', 'considered', '.', 'solutions', 'for', 'the', 'boundarylayer', 'thickness', ',', 'skin', 'friction', ',', 'and', 'the', 'velocity', 'distribution', 'in', 'the', 'boundary', 'layer', 'are', 'obtained', 'by', 'the', 'karman-pohlhausen', 'technique', '.', 'comparison', 'with', 'the', 'boundary', 'layer', 'of', 'a', 'uniform', 'flow', 'has', 'also', 'been', 'made', 'to', 'show', 'the', 'effect', 'of', 'vorticity', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['one-dimensional', 'transient', 'heat', 'conduction', 'into', 'a', 'double-layer', 'slab', 'subjected', 'to', 'a', 'linear', 'heat', 'input', 'for', 'a', 'small', 'time', 'internal', '.', 'analytic', 'solutions', 'are', 'presented', 'for', 'the', 'transient', 'heat', 'conduction', 'in', 'composite', 'slabs', 'exposed', 'at', 'one', 'surface', 'to', 'a', 'triangular', 'heat', 'rate', '.', 'this', 'type', 'of', 'heating', 'rate', 'may', 'occur', ',', 'for', 'example', ',', 'during', 'aerodynamic', 'heating', '.']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_set = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "DYuFaUWpU96X"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Remove stopwords"
      ],
      "metadata": {
        "id": "Wy1mdSR-VVjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  tokens_sans_stopwords = remove_stopwords_from_tokens(filedata, stopwords_set)\n",
        "  newfiles[i] = tokens_sans_stopwords\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX7AzcaMVTlC",
        "outputId": "82030e6f-2312-4e6c-dbe6-3e89f59bf073"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', '.', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution', 'lift', 'increase', 'due', 'slipstream', 'different', 'angles', 'attack', 'wing', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', '.', 'results', 'intended', 'part', 'evaluation', 'basis', 'different', 'theoretical', 'treatments', 'problem', '.', 'comparative', 'span', 'loading', 'curves', ',', 'together', 'supporting', 'evidence', ',', 'showed', 'substantial', 'part', 'lift', 'increment', 'produced', 'slipstream', 'due', '/destalling/', 'boundary-layer-control', 'effect', '.', 'integrated', 'remaining', 'lift', 'increment', ',', 'subtracting', 'destalling', 'lift', ',', 'found', 'agree', 'well', 'potential', 'flow', 'theory', '.', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'specific', 'configuration', 'experiment', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', '.', 'study', 'high-speed', 'viscous', 'flow', 'past', 'two-dimensional', 'body', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'nose', 'leading', 'edge', 'body', '.', 'consequently', ',', 'exists', 'inviscid', 'rotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', '.', 'situation', 'arises', ',', 'instance', ',', 'study', 'hypersonic', 'viscous', 'flow', 'past', 'flat', 'plate', '.', 'situation', 'somewhat', 'different', 'prandtl', \"'s\", 'classical', 'boundary-layer', 'problem', '.', \"prandtl's\", 'original', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'irrotational', 'hypersonic', 'boundary-layer', 'problem', 'inviscid', 'free', 'stream', 'must', 'considered', 'rotational', '.', 'possible', 'effects', 'vorticity', 'recently', 'discussed', 'ferri', 'libby', '.', 'present', 'paper', ',', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'small', 'viscosity', 'investigated', '.', 'shown', 'problem', 'treated', 'boundary-layer', 'approximation', ',', 'novel', 'feature', 'free', 'stream', 'constant', 'vorticity', '.', 'discussion', 'restricted', 'two-dimensional', 'incompressible', 'steady', 'flow', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['boundary', 'layer', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', '.', 'boundary-layer', 'equations', 'presented', 'steady', 'incompressible', 'flow', 'pressure', 'gradient', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'plate', 'shear', 'flow', '.', 'two-dimensional', 'steady', 'boundary-layer', 'problem', 'flat', 'plate', 'shear', 'flow', 'incompressible', 'fluid', 'considered', '.', 'solutions', 'boundarylayer', 'thickness', ',', 'skin', 'friction', ',', 'velocity', 'distribution', 'boundary', 'layer', 'obtained', 'karman-pohlhausen', 'technique', '.', 'comparison', 'boundary', 'layer', 'uniform', 'flow', 'also', 'made', 'show', 'effect', 'vorticity', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['one-dimensional', 'transient', 'heat', 'conduction', 'double-layer', 'slab', 'subjected', 'linear', 'heat', 'input', 'small', 'time', 'internal', '.', 'analytic', 'solutions', 'presented', 'transient', 'heat', 'conduction', 'composite', 'slabs', 'exposed', 'one', 'surface', 'triangular', 'heat', 'rate', '.', 'type', 'heating', 'rate', 'may', 'occur', ',', 'example', ',', 'aerodynamic', 'heating', '.']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Remove punctuations"
      ],
      "metadata": {
        "id": "5aW5_CJaVdvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  tokens_sans_punctuation = remove_punctuation_from_tokens(filedata)\n",
        "  newfiles[i] = tokens_sans_punctuation\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkVA-WvnVbAe",
        "outputId": "357d3fce-3956-487b-9148-81beec85cf83"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', '', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution', 'lift', 'increase', 'due', 'slipstream', 'different', 'angles', 'attack', 'wing', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', '', 'results', 'intended', 'part', 'evaluation', 'basis', 'different', 'theoretical', 'treatments', 'problem', '', 'comparative', 'span', 'loading', 'curves', '', 'together', 'supporting', 'evidence', '', 'showed', 'substantial', 'part', 'lift', 'increment', 'produced', 'slipstream', 'due', 'destalling', 'boundarylayercontrol', 'effect', '', 'integrated', 'remaining', 'lift', 'increment', '', 'subtracting', 'destalling', 'lift', '', 'found', 'agree', 'well', 'potential', 'flow', 'theory', '', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'specific', 'configuration', 'experiment', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', '', 'study', 'highspeed', 'viscous', 'flow', 'past', 'twodimensional', 'body', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'nose', 'leading', 'edge', 'body', '', 'consequently', '', 'exists', 'inviscid', 'rotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', '', 'situation', 'arises', '', 'instance', '', 'study', 'hypersonic', 'viscous', 'flow', 'past', 'flat', 'plate', '', 'situation', 'somewhat', 'different', 'prandtl', 's', 'classical', 'boundarylayer', 'problem', '', 'prandtls', 'original', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'irrotational', 'hypersonic', 'boundarylayer', 'problem', 'inviscid', 'free', 'stream', 'must', 'considered', 'rotational', '', 'possible', 'effects', 'vorticity', 'recently', 'discussed', 'ferri', 'libby', '', 'present', 'paper', '', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'small', 'viscosity', 'investigated', '', 'shown', 'problem', 'treated', 'boundarylayer', 'approximation', '', 'novel', 'feature', 'free', 'stream', 'constant', 'vorticity', '', 'discussion', 'restricted', 'twodimensional', 'incompressible', 'steady', 'flow', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['boundary', 'layer', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', '', 'boundarylayer', 'equations', 'presented', 'steady', 'incompressible', 'flow', 'pressure', 'gradient', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'plate', 'shear', 'flow', '', 'twodimensional', 'steady', 'boundarylayer', 'problem', 'flat', 'plate', 'shear', 'flow', 'incompressible', 'fluid', 'considered', '', 'solutions', 'boundarylayer', 'thickness', '', 'skin', 'friction', '', 'velocity', 'distribution', 'boundary', 'layer', 'obtained', 'karmanpohlhausen', 'technique', '', 'comparison', 'boundary', 'layer', 'uniform', 'flow', 'also', 'made', 'show', 'effect', 'vorticity', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['onedimensional', 'transient', 'heat', 'conduction', 'doublelayer', 'slab', 'subjected', 'linear', 'heat', 'input', 'small', 'time', 'internal', '', 'analytic', 'solutions', 'presented', 'transient', 'heat', 'conduction', 'composite', 'slabs', 'exposed', 'one', 'surface', 'triangular', 'heat', 'rate', '', 'type', 'heating', 'rate', 'may', 'occur', '', 'example', '', 'aerodynamic', 'heating', '']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Remove blank space tokens"
      ],
      "metadata": {
        "id": "74MvPUS4Vi2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lent = 0\n",
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  tokens_sans_blank_space = remove_blank_space_tokens(filedata)\n",
        "  newfiles[i] = tokens_sans_blank_space\n",
        "  lent += len(newfiles[i])\n",
        "print(len(newfiles))\n",
        "print(lent)\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gskynoymVgHk",
        "outputId": "109b1143-67c3-4d16-b2ff-48bccfb102d9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "127377\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution', 'lift', 'increase', 'due', 'slipstream', 'different', 'angles', 'attack', 'wing', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', 'results', 'intended', 'part', 'evaluation', 'basis', 'different', 'theoretical', 'treatments', 'problem', 'comparative', 'span', 'loading', 'curves', 'together', 'supporting', 'evidence', 'showed', 'substantial', 'part', 'lift', 'increment', 'produced', 'slipstream', 'due', 'destalling', 'boundarylayercontrol', 'effect', 'integrated', 'remaining', 'lift', 'increment', 'subtracting', 'destalling', 'lift', 'found', 'agree', 'well', 'potential', 'flow', 'theory', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'specific', 'configuration', 'experiment']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', 'study', 'highspeed', 'viscous', 'flow', 'past', 'twodimensional', 'body', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'nose', 'leading', 'edge', 'body', 'consequently', 'exists', 'inviscid', 'rotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', 'situation', 'arises', 'instance', 'study', 'hypersonic', 'viscous', 'flow', 'past', 'flat', 'plate', 'situation', 'somewhat', 'different', 'prandtl', 's', 'classical', 'boundarylayer', 'problem', 'prandtls', 'original', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'irrotational', 'hypersonic', 'boundarylayer', 'problem', 'inviscid', 'free', 'stream', 'must', 'considered', 'rotational', 'possible', 'effects', 'vorticity', 'recently', 'discussed', 'ferri', 'libby', 'present', 'paper', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'small', 'viscosity', 'investigated', 'shown', 'problem', 'treated', 'boundarylayer', 'approximation', 'novel', 'feature', 'free', 'stream', 'constant', 'vorticity', 'discussion', 'restricted', 'twodimensional', 'incompressible', 'steady', 'flow']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['boundary', 'layer', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'boundarylayer', 'equations', 'presented', 'steady', 'incompressible', 'flow', 'pressure', 'gradient']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'plate', 'shear', 'flow', 'twodimensional', 'steady', 'boundarylayer', 'problem', 'flat', 'plate', 'shear', 'flow', 'incompressible', 'fluid', 'considered', 'solutions', 'boundarylayer', 'thickness', 'skin', 'friction', 'velocity', 'distribution', 'boundary', 'layer', 'obtained', 'karmanpohlhausen', 'technique', 'comparison', 'boundary', 'layer', 'uniform', 'flow', 'also', 'made', 'show', 'effect', 'vorticity']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['onedimensional', 'transient', 'heat', 'conduction', 'doublelayer', 'slab', 'subjected', 'linear', 'heat', 'input', 'small', 'time', 'internal', 'analytic', 'solutions', 'presented', 'transient', 'heat', 'conduction', 'composite', 'slabs', 'exposed', 'one', 'surface', 'triangular', 'heat', 'rate', 'type', 'heating', 'rate', 'may', 'occur', 'example', 'aerodynamic', 'heating']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fileData = newfiles"
      ],
      "metadata": {
        "id": "aWPNZytWVkei"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF Matrix [25 points]"
      ],
      "metadata": {
        "id": "8T0tr-gmWyGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining helper functions"
      ],
      "metadata": {
        "id": "_SeP56npXUwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_file_dictionary(list_of_files):\n",
        "    '''\n",
        "    Paramteres:\n",
        "        list_of_files: type(string)\n",
        "    \n",
        "    returns: file_dictionary with integer key and path_of_file as value\n",
        "    '''\n",
        "    file_dictionary = {}\n",
        "    for i in range(len(list_of_files)):\n",
        "        file_dictionary[i] = list_of_files[i]\n",
        "    \n",
        "    return file_dictionary"
      ],
      "metadata": {
        "id": "1W2DO-lqWEs8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_2_way_mapping_of_term_and_doc(global_vocabulary):\n",
        "    '''\n",
        "    Parameter:\n",
        "        global_vocabulary: type(list)\n",
        "    \n",
        "    returns: 2 dictionaries with key-value pair as term:docID and docID:term respectively\n",
        "    '''\n",
        "    term_vs_ID_dict = {}\n",
        "    ID_vs_term_dict = {}\n",
        "    \n",
        "    for idx,term in enumerate(global_vocabulary):\n",
        "        ID_vs_term_dict[idx] = term\n",
        "        term_vs_ID_dict[term] = idx\n",
        "    \n",
        "    return term_vs_ID_dict, ID_vs_term_dict\n",
        "\n",
        "\n",
        "'''\n",
        "This code creates two dictionaries that will be used to map terms to their corresponding index in a matrix and vice versa.\n",
        "\n",
        "The first dictionary, term_vs_ID_dict, maps each term in the global vocabulary to its corresponding index in the matrix. The keys in this dictionary are the terms themselves, and the values are the corresponding indices.\n",
        "\n",
        "The second dictionary, ID_vs_term_dict, maps each index in the matrix to its corresponding term in the global vocabulary. The keys in this dictionary are the indices, and the values are the corresponding terms.\n",
        "\n",
        "The for loop iterates over each term in the global_vocabulary list and assigns an index to it. The enumerate function is used to get both the index and the corresponding term in each iteration. The index is used as the key in the ID_vs_term_dict dictionary, and the term is used as the key in the term_vs_ID_dict dictionary.\n",
        "'''"
      ],
      "metadata": {
        "id": "nAMbdqemXbdF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "a370ad67-15f1-48fc-be82-19bfd2eb28f9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThis code creates two dictionaries that will be used to map terms to their corresponding index in a matrix and vice versa.\\n\\nThe first dictionary, term_vs_ID_dict, maps each term in the global vocabulary to its corresponding index in the matrix. The keys in this dictionary are the terms themselves, and the values are the corresponding indices.\\n\\nThe second dictionary, ID_vs_term_dict, maps each index in the matrix to its corresponding term in the global vocabulary. The keys in this dictionary are the indices, and the values are the corresponding terms.\\n\\nThe for loop iterates over each term in the global_vocabulary list and assigns an index to it. The enumerate function is used to get both the index and the corresponding term in each iteration. The index is used as the key in the ID_vs_term_dict dictionary, and the term is used as the key in the term_vs_ID_dict dictionary.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_total_words_in_doc(TF_IDF_doc_vs_term_matrix):  # this function calculates the total number of words in each document represented by a given TF-IDF matrix.\n",
        "    return np.sum(TF_IDF_doc_vs_term_matrix, axis=1)    # axis=1 means row wise sum"
      ],
      "metadata": {
        "id": "YSdU3wsqXgsm"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_freq_term_in_doc(matrix):   # this function returns a 1D array containing the maximum value for each row of the input matrix. In other words, it returns an array of the maximum frequency terms in each document represented by the rows of the input matrix.\n",
        "    return np.amax(matrix, axis=1)"
      ],
      "metadata": {
        "id": "-Xl4VE0hXmfp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_IDF(inverted_index, total_documents):   # this function returns the IDF dictionary, which contains the IDF values for each term in the inverted_index. The IDF values can be used in information retrieval tasks to weight the importance of each term in a document or query.\n",
        "    IDF = {}\n",
        "    for term in inverted_index:\n",
        "        IDF[term] = np.log10(total_documents/(1 + len(inverted_index[term])))\n",
        "    return IDF"
      ],
      "metadata": {
        "id": "iwXsz7toXory"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Binary_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix):    # The function uses NumPy's \"where\" function to set values in the matrix to either 0 or 1. The condition in the \"where\" function is (TF_IDF_doc_vs_term_matrix <= 0), which checks whether the TF-IDF weight in each cell is less than or equal to 0. If it is, the value in that cell is set to 0; otherwise, it is set to 1.\n",
        "    '''\n",
        "    Parameters:\n",
        "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
        "    \n",
        "    returns: binary weighting scheme\n",
        "    '''\n",
        "    return np.where((TF_IDF_doc_vs_term_matrix <= 0),0,1).astype('float')"
      ],
      "metadata": {
        "id": "fYYomfLEXr1e"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Raw_count_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix):   # The function simply returns the input matrix, which means that the raw count weighting scheme is not applied to the matrix. Therefore, the output matrix will be the same as the input matrix, containing the TF-IDF weights of each term in each document. In other words, the function does not modify the input matrix and returns it as is.\n",
        "    '''\n",
        "    Parameters:\n",
        "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
        "    \n",
        "    returns: raw count weighing scheme\n",
        "    '''\n",
        "    return TF_IDF_doc_vs_term_matrix"
      ],
      "metadata": {
        "id": "WpFWL-XEXv4d"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Term_frequency_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix, total_words_in_doc):    # The function first transposes the input matrix using TF_IDF_doc_vs_term_matrix.T, so that the rows correspond to each document and the columns correspond to each term. It then divides each element in each row by the corresponding value in total_words_in_doc to obtain the TF value for each term in each document. The resulting matrix is then transposed back to the original shape, with rows corresponding to each term and columns corresponding to each document. Finally, the function returns the resulting term frequency weighting scheme as a numpy array.\n",
        "    '''\n",
        "    Parameters:\n",
        "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
        "        total_words_in_doc: type(np.array)\n",
        "    \n",
        "    returns: term frequency weighing scheme\n",
        "    '''\n",
        "    return (TF_IDF_doc_vs_term_matrix.T / total_words_in_doc).T"
      ],
      "metadata": {
        "id": "ESpvsDNFdO2P"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Log_Normalization_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix):   # Log normalization is a method used to transform the TF values in a TF-IDF matrix to reduce the impact of very frequent terms and increase the impact of rare terms. This helps to address the issue of common words having a high impact on the similarity calculation even though they may not carry much meaning. By applying a logarithmic transformation to the TF values, the values for very frequent terms will be reduced, while the values for rare terms will be increased. This is because the logarithmic function is an increasing function that grows more slowly as the input value increases. As a result, the impact of the most frequent terms is scaled down, while the impact of the less frequent terms is scaled up.\n",
        "    '''\n",
        "    Parameters:\n",
        "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
        "    \n",
        "    returns: log normalization weighing scheme\n",
        "    '''\n",
        "    return np.log10(1+TF_IDF_doc_vs_term_matrix)"
      ],
      "metadata": {
        "id": "qyYwDiaQX3y3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Double_Normalization_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix, max_freq_term_in_doc):    # The function first transposes the input matrix TF_IDF_doc_vs_term_matrix and divides each row by the corresponding maximum frequency of any term in that document. This is done to normalize the values in the matrix. After normalization, the function applies the double normalization weighing scheme by multiplying the resulting matrix by a constant of 0.5 and adding 0.5 to it. This scaling of values between 0 and 1 is used to represent the relative importance of each term in each document.\n",
        "    '''\n",
        "    Parameters:\n",
        "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
        "    \n",
        "    returns: double normalization weighing scheme\n",
        "    '''\n",
        "    matrix = (TF_IDF_doc_vs_term_matrix.T / max_freq_term_in_doc).T\n",
        "    value = 0.5 + (0.5*matrix)\n",
        "\n",
        "    return value"
      ],
      "metadata": {
        "id": "KgvBC9lwX61J"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_variants(matrix, total_terms, max_freq, size_of_global_vocabulary, IDF, ID_vs_term_dict, isQuery):\n",
        "    '''\n",
        "    returns: variants of TF-IDF\n",
        "    '''\n",
        "    Binary = Binary_weighing_TF_IDF(matrix)\n",
        "    Raw_count = Raw_count_weighing_TF_IDF(matrix)\n",
        "    Term_frequency = Term_frequency_weighing_TF_IDF(matrix, total_terms)\n",
        "    Log_Normalization = Log_Normalization_weighing_TF_IDF(matrix)\n",
        "    Double_Normalization = Double_Normalization_weighing_TF_IDF(matrix, max_freq)\n",
        "    \n",
        "    if(isQuery):\n",
        "        for i in tqdm(range(size_of_global_vocabulary)):\n",
        "            IDF_factor = IDF[ID_vs_term_dict[i]]\n",
        "            Binary[i] *= IDF_factor\n",
        "            Raw_count[i] *= IDF_factor\n",
        "            Term_frequency[i] *= IDF_factor\n",
        "            Log_Normalization[i] *= IDF_factor\n",
        "            Double_Normalization[i] *= IDF_factor\n",
        "    else:\n",
        "        for i in tqdm(range(size_of_global_vocabulary)):\n",
        "            IDF_factor = IDF[ID_vs_term_dict[i]]\n",
        "            Binary[:,i] *= IDF_factor\n",
        "            Raw_count[:,i] *= IDF_factor\n",
        "            Term_frequency[:,i] *= IDF_factor\n",
        "            Log_Normalization[:, i] *= IDF_factor\n",
        "            Double_Normalization[:, i] *= IDF_factor\n",
        "    \n",
        "    return Binary, Raw_count, Term_frequency, Log_Normalization, Double_Normalization"
      ],
      "metadata": {
        "id": "QzWNmUw7X9Hn"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topN(N, TF_IDF, query_of_same_variant, file_dictionary):    # Here's a breakdown of what it means: relevant_docs is a variable that will store the output of the code. list() is a Python function that creates a new list object. sorted() is another Python function that sorts the items in an iterable object (such as a dictionary) and returns a new list. tf_idf_score.items() is a Python dictionary method that returns a view object that contains the key-value pairs of the dictionary. key=operator.itemgetter(1) specifies that the sort order should be based on the value of the dictionary items (the second element in each key-value pair). reverse=True specifies that the sort order should be in descending order (i.e., highest to lowest). [:N] is Python syntax for slicing a list. It specifies that only the first N elements of the sorted list should be included in the output. So, in summary, this line of code takes a dictionary called tf_idf_score that contains key-value pairs, where the keys are document IDs and the values are TF-IDF scores, and sorts the dictionary by the values (in descending order) and returns the top N documents (specified by the N variable) with the highest TF-IDF scores in a new list called relevant_docs.\n",
        "    '''\n",
        "    evaluates and displays top N relevant documents based on score\n",
        "    '''\n",
        "    tf_idf_score = np.dot(TF_IDF, query_of_same_variant)\n",
        "    tf_idf_score = {file_dictionary[i]:tf_idf_score[i] for i in range(len(tf_idf_score))}\n",
        "    \n",
        "    relevant_docs = list(sorted(tf_idf_score.items(), key=operator.itemgetter(1),reverse=True))[:N]\n",
        "    for docs in relevant_docs:\n",
        "        print('Score: {}  Document: {}'.format(docs[1], docs[0]))\n",
        "    print()"
      ],
      "metadata": {
        "id": "4P35L72QYHF_"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAIN"
      ],
      "metadata": {
        "id": "9I4rnbRxYPB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # create set of stop words for preprocessing\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    \n",
        "    # Get List of Files in Dataset\n",
        "    list_of_files = getListOfFiles('/content/CSE508_Winter2023_Dataset')\n",
        "    \n",
        "    # create dictionary of file with docID (integer) as key and full_path of file as value\n",
        "    file_dictionary = create_file_dictionary(list_of_files)\n",
        "    total_documents = len(file_dictionary)\n",
        "    \n",
        "    preprocessed_list_of_docs_tokens = preprocess_documents(list_of_files, stopwords_set)\n",
        "    # pi_file = open('Q1_tf_idf.pkl', 'rb')\n",
        "    # preprocessed_list_of_docs_tokens = pickle.load(pi_file)\n",
        "    # pi_file.close()\n",
        "    \n",
        "    # Global list of terms\n",
        "    global_list_of_terms = []\n",
        "    for doc in preprocessed_list_of_docs_tokens:\n",
        "        global_list_of_terms.extend(doc)\n",
        "    \n",
        "    # finding all distinct terms across all documents\n",
        "    global_vocabulary = list(set(global_list_of_terms))\n",
        "    size_of_global_vocabulary = len(global_vocabulary)\n",
        "    \n",
        "    # term vs docID 2 way mapping\n",
        "    term_vs_ID_dict, ID_vs_term_dict = create_2_way_mapping_of_term_and_doc(global_vocabulary)\n",
        "    \n",
        "    # term-doc matrix (for storing frequency of each word of global_vocabulary in doc)\n",
        "    inverted_index = {}\n",
        "    size_of_TF_IDF_matrix = (total_documents, size_of_global_vocabulary)                      # Create a matrix of size no. of documents x vocab size.    (RUBRIC)\n",
        "    TF_IDF_doc_vs_term_matrix = np.zeros(size_of_TF_IDF_matrix, dtype=float)\n",
        "    \n",
        "    for i in tqdm(range(len(preprocessed_list_of_docs_tokens))):\n",
        "        for term in preprocessed_list_of_docs_tokens[i]:\n",
        "            if(term in inverted_index):\n",
        "                if(inverted_index[term][-1]!=i):\n",
        "                    inverted_index[term].append(i)\n",
        "            else:\n",
        "                inverted_index[term] = [i]\n",
        "            TF_IDF_doc_vs_term_matrix[i][term_vs_ID_dict[term]] += 1\n",
        "    \n",
        "    total_words_in_doc = get_total_words_in_doc(TF_IDF_doc_vs_term_matrix)    # this function calculates the total number of words in each document represented by a given TF-IDF matrix.\n",
        "    \n",
        "    max_freq_term_in_doc = get_max_freq_term_in_doc(TF_IDF_doc_vs_term_matrix)    # this function returns a 1D array containing the maximum value for each row of the input matrix. In other words, it returns an array of the maximum frequency terms in each document represented by the rows of the input matrix.\n",
        "    \n",
        "    # Calculate IDF\n",
        "    IDF = calculate_IDF(inverted_index, total_documents)\n",
        "    \n",
        "    # Fill in the tf-idf values for each term in the vocabulary in the matrix.    (RUBRIC)\n",
        "    Binary_tf_idf, Raw_count_tf_idf, Term_frequency_tf_idf, Log_Normalization_tf_idf, Double_Normalization_tf_idf = compute_variants(TF_IDF_doc_vs_term_matrix, total_words_in_doc, max_freq_term_in_doc, size_of_global_vocabulary, IDF, ID_vs_term_dict, False)\n",
        "    \n",
        "    query = input(\"Input query: \")\n",
        "    sanitized_query = preprocess(query, stopwords_set)\n",
        "    print(\"Sanitized query: \", sanitized_query)\n",
        "\n",
        "    query_frequency = {}\n",
        "    for query_token in sanitized_query:\n",
        "        if query_token in query_frequency:\n",
        "            query_frequency[query_token]+=1\n",
        "        else:\n",
        "            query_frequency[query_token]=1\n",
        "    \n",
        "    # create query frequency vector\n",
        "    query_frequency_vector = np.zeros((size_of_global_vocabulary,1))                        # Construct the query vector of size vocab.   (RUBRIC)\n",
        "    for token in query_frequency.keys():\n",
        "        if token in global_vocabulary:\n",
        "            query_frequency_vector[term_vs_ID_dict[token]] = query_frequency[token]\n",
        "    \n",
        "    \n",
        "    max_freq_token = query_frequency[max(query_frequency, key=query_frequency.get)]\n",
        "    # print(max(query_frequency, key=query_frequency.get), max_freq_token, \"just to test\")\n",
        "\n",
        "    # Compute the TF-IDF score for the query using the TF-IDF matrix. Report the top 5 relevant documents based on the score.   (RUBRIC)\n",
        "    query_binary, query_raw_count, query_term_frequency, query_log_normalization, query_double_normalization = compute_variants(query_frequency_vector, len(sanitized_query), max_freq_token, size_of_global_vocabulary, IDF, ID_vs_term_dict, True)\n",
        "    \n",
        "\n",
        "    # Use all 5 weighting schemes for term frequency calculation and report the TF-IDF score and results for each scheme separately.    (RUBRIC)\n",
        "    print('\\n\\nBinary Scheme: Top 5 relevant documents are:')\n",
        "    topN(5, Binary_tf_idf, query_binary, file_dictionary)\n",
        "    print(Binary_tf_idf, \"\\n\\n\")\n",
        "    \n",
        "    print('Raw Count Scheme: Top 5 relevant documents are:')\n",
        "    topN(5, Raw_count_tf_idf, query_raw_count, file_dictionary)\n",
        "    print(Raw_count_tf_idf, \"\\n\\n\")\n",
        "    \n",
        "    print('Term Frequency Scheme: Top 5 relevant documents are:')\n",
        "    topN(5, Term_frequency_tf_idf, query_term_frequency, file_dictionary)\n",
        "    print(Term_frequency_tf_idf, \"\\n\\n\")\n",
        "    \n",
        "    print('Log Normalization Scheme: Top 5 relevant documents are:')\n",
        "    topN(5, Log_Normalization_tf_idf, query_log_normalization, file_dictionary)\n",
        "    print(Log_Normalization_tf_idf, \"\\n\\n\")\n",
        "    \n",
        "    print('Double_Normalization Scheme: Top 5 relevant documents are:')\n",
        "    topN(5, Double_Normalization_tf_idf, query_double_normalization, file_dictionary)\n",
        "    print(Double_Normalization_tf_idf, \"\\n\\n\")"
      ],
      "metadata": {
        "id": "MG0utMO-YLbG"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"cosmonautics is currently very much to the forefront in the news\"    # cranfield718"
      ],
      "metadata": {
        "id": "PkHlBCfHcoT_"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVPbv2AMYbh3",
        "outputId": "91a6023d-c6e7-411d-f6f8-03a6b7d5c780"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1400/1400 [00:00<00:00, 4063.16it/s]\n",
            "100%|| 8996/8996 [00:00<00:00, 14186.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input query: cosmonautics is currently very much to the forefront in the news\n",
            "Sanitized query:  ['cosmonautics', 'currently', 'much', 'forefront', 'news']\n",
            "cosmonautics 1 YOUR ERROR COMES HERE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 8996/8996 [00:00<00:00, 32962.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Binary Scheme: Top 5 relevant documents are:\n",
            "Score: [30.1560637]  Document: /content/CSE508_Winter2023_Dataset/cranfield0718\n",
            "Score: [4.12977459]  Document: /content/CSE508_Winter2023_Dataset/cranfield0092\n",
            "Score: [4.12977459]  Document: /content/CSE508_Winter2023_Dataset/cranfield0115\n",
            "Score: [4.12977459]  Document: /content/CSE508_Winter2023_Dataset/cranfield0367\n",
            "Score: [4.12977459]  Document: /content/CSE508_Winter2023_Dataset/cranfield0506\n",
            "\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] \n",
            "\n",
            "\n",
            "Raw Count Scheme: Top 5 relevant documents are:\n",
            "Score: [31.89860423]  Document: /content/CSE508_Winter2023_Dataset/cranfield0718\n",
            "Score: [5.22762161]  Document: /content/CSE508_Winter2023_Dataset/cranfield0114\n",
            "Score: [4.12977459]  Document: /content/CSE508_Winter2023_Dataset/cranfield0092\n",
            "Score: [4.12977459]  Document: /content/CSE508_Winter2023_Dataset/cranfield0115\n",
            "Score: [4.12977459]  Document: /content/CSE508_Winter2023_Dataset/cranfield0367\n",
            "\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] \n",
            "\n",
            "\n",
            "Term Frequency Scheme: Top 5 relevant documents are:\n",
            "Score: [0.08075596]  Document: /content/CSE508_Winter2023_Dataset/cranfield0718\n",
            "Score: [0.02294319]  Document: /content/CSE508_Winter2023_Dataset/cranfield0909\n",
            "Score: [0.02232311]  Document: /content/CSE508_Winter2023_Dataset/cranfield1111\n",
            "Score: [0.01354024]  Document: /content/CSE508_Winter2023_Dataset/cranfield1052\n",
            "Score: [0.01332185]  Document: /content/CSE508_Winter2023_Dataset/cranfield0506\n",
            "\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] \n",
            "\n",
            "\n",
            "Log Normalization Scheme: Top 5 relevant documents are:\n",
            "Score: [2.82508399]  Document: /content/CSE508_Winter2023_Dataset/cranfield0718\n",
            "Score: [0.37423628]  Document: /content/CSE508_Winter2023_Dataset/cranfield0092\n",
            "Score: [0.37423628]  Document: /content/CSE508_Winter2023_Dataset/cranfield0115\n",
            "Score: [0.37423628]  Document: /content/CSE508_Winter2023_Dataset/cranfield0367\n",
            "Score: [0.37423628]  Document: /content/CSE508_Winter2023_Dataset/cranfield0506\n",
            "\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] \n",
            "\n",
            "\n",
            "Double_Normalization Scheme: Top 5 relevant documents are:\n",
            "Score: [14682.28531238]  Document: /content/CSE508_Winter2023_Dataset/cranfield0718\n",
            "Score: [14666.13334089]  Document: /content/CSE508_Winter2023_Dataset/cranfield0212\n",
            "Score: [14661.83758148]  Document: /content/CSE508_Winter2023_Dataset/cranfield1167\n",
            "Score: [14661.71579585]  Document: /content/CSE508_Winter2023_Dataset/cranfield0344\n",
            "Score: [14661.31333709]  Document: /content/CSE508_Winter2023_Dataset/cranfield1088\n",
            "\n",
            "[[1.42254902 1.42254902 1.42254902 ... 0.60581479 1.42254902 1.33450339]\n",
            " [1.42254902 1.42254902 1.42254902 ... 0.60581479 1.42254902 1.33450339]\n",
            " [1.42254902 1.42254902 1.42254902 ... 0.60581479 1.42254902 1.33450339]\n",
            " ...\n",
            " [1.42254902 1.42254902 1.42254902 ... 0.60581479 1.42254902 1.33450339]\n",
            " [1.42254902 1.42254902 1.42254902 ... 0.60581479 1.42254902 1.33450339]\n",
            " [1.42254902 1.42254902 1.42254902 ... 0.60581479 1.42254902 1.33450339]] \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Jaccard Coefficient [15 marks]"
      ],
      "metadata": {
        "id": "EaWz9MARe-EU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Union function"
      ],
      "metadata": {
        "id": "TiQAD1jlgLCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def union(l1,l2):\n",
        "    # union_list = (set(l1) | set(l2))\n",
        "    l1 = set(l1)\n",
        "    l2 = set(l2)\n",
        "    return l1.union(l2)"
      ],
      "metadata": {
        "id": "HfM1ymDGcl5A"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intersection function"
      ],
      "metadata": {
        "id": "sO6JvPMmgScW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection(l1,l2):\n",
        "    # intersection_list = (set(l1) & set(l2))\n",
        "    # return intersection_list\n",
        "    l1 = set(l1)\n",
        "    l2 = set(l2)\n",
        "    return l1.intersection(l2)"
      ],
      "metadata": {
        "id": "0LL4cP_PgNF9"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Jaccard coefficient"
      ],
      "metadata": {
        "id": "d5h5MHaMgZSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccardCoefficient(query, list_of_files):\n",
        "    for i,filePath in enumerate (list_of_files):\n",
        "        file = open(filePath, encoding=\"utf8\", errors = \"ignore\")\n",
        "        read = file.read()    \n",
        "        file.close()\n",
        "    \n",
        "        sanitized_query = preprocess(read, stopwords_set)                    \n",
        "        # calculate jaccard coefficient value based on formula that is intersection of document and query divided by union of \n",
        "        # document and query\n",
        "        jaccard_coefficient[filePath] = len(intersection(sanitized_query, query))/len(union(sanitized_query, query))"
      ],
      "metadata": {
        "id": "LJrNuDt9gYNy"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJaAWIuXhzzQ",
        "outputId": "6170cf4b-a0c8-4038-d193-fe6ec5cdf1a0"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding top 10 relevant documents"
      ],
      "metadata": {
        "id": "t2zs2m4ogjtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"cosmonautics is currently very much to the forefront in the news\""
      ],
      "metadata": {
        "id": "jp4ekVaRdS43"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get List of Files in Dataset\n",
        "list_of_files = getListOfFiles(\"/content/CSE508_Winter2023_Dataset/\")\n",
        "#print(list_of_files)\n",
        "jaccard_coefficient = {}        \n",
        "sentence_query = input(\"Enter the query: \")\n",
        "query = preprocess(sentence_query, stopwords_set)      # Query Processing\n",
        "jaccardCoefficient(query, list_of_files)\n",
        "# it counts the elements in the dictionary and prints the 5 most common documents based on it.\n",
        "#k = Counter(dict(jaccard_coefficient)).most_common(5) \n",
        "print(\" Top 10 relevant documents based on Jaccard Coefficient \")  \n",
        "for i in Counter(dict(jaccard_coefficient)).most_common(10) :\n",
        "    print(\"{} --> {}\".format(i[1],i[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5HuOLKugWKM",
        "outputId": "5991ac5e-400e-41ca-9985-b97e020d8ba9"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the query: cosmonautics is currently very much to the forefront in the news\n",
            " Top 10 relevant documents based on Jaccard Coefficient \n",
            "0.07246376811594203 --> /content/CSE508_Winter2023_Dataset/cranfield0718\n",
            "0.02857142857142857 --> /content/CSE508_Winter2023_Dataset/cranfield0909\n",
            "0.027777777777777776 --> /content/CSE508_Winter2023_Dataset/cranfield1111\n",
            "0.023809523809523808 --> /content/CSE508_Winter2023_Dataset/cranfield1369\n",
            "0.021739130434782608 --> /content/CSE508_Winter2023_Dataset/cranfield0840\n",
            "0.02 --> /content/CSE508_Winter2023_Dataset/cranfield1033\n",
            "0.019230769230769232 --> /content/CSE508_Winter2023_Dataset/cranfield1396\n",
            "0.018518518518518517 --> /content/CSE508_Winter2023_Dataset/cranfield0383\n",
            "0.01818181818181818 --> /content/CSE508_Winter2023_Dataset/cranfield0866\n",
            "0.01818181818181818 --> /content/CSE508_Winter2023_Dataset/cranfield1052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# queries -> jet propulsion, supersonic wind tunnel, experimental study of a wing in a propeller slipstream, differential equations for a heated plate with large temperature gradient <cranfield13>, there are in supersonic aerodynamics many situations of practical interest wherein streams of different velocities and, in general <cranfield11>"
      ],
      "metadata": {
        "id": "UqDVck2NiBP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "julXORT6By5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wkFZYNdUBy3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HsfmxenwBh19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ct-wX3suBh0J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}