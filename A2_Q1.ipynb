{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFyLauAJI2RiSwJl2Esfig",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammed-Taasir/CSE508_Winter2023_A2_14/blob/main/A2_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K6yWjgMRM3YV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ebb8c01-705b-4694-a742-c6737d818e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/MyDrive/IR Assignment-1/CSE508_Winter2023_Dataset.zip' '/content/'\n",
        "!unzip 'CSE508_Winter2023_Dataset.zip' &> /dev/null\n",
        "!rm 'CSE508_Winter2023_Dataset.zip'"
      ],
      "metadata": {
        "id": "liUhbCY4RzoP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/CSE508_Winter2023_Dataset/'"
      ],
      "metadata": {
        "id": "vqIaHYCjSJfS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "import random\n",
        "import math\n",
        "import heapq\n",
        "import operator\n",
        "from collections import Counter\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "qqARjmOqTB7a"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sortedcontainers import SortedDict, SortedList, SortedSet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import PlaintextCorpusReader \n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "9hMpK8l_TWXP"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Vjwl0_ZTYZP",
        "outputId": "a0ac10f0-727d-4a81-c8c5-c28c63f3c3ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# * Defining Helper Functions"
      ],
      "metadata": {
        "id": "XKJzHpviTlJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read from files"
      ],
      "metadata": {
        "id": "4_Bv4U1ETn0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  def getListOfFiles(directory):\n",
        "\n",
        "    # Parameters: directory: type(string)        \n",
        "    # returns: list of all files in directory with the full path of file\n",
        "    \n",
        "    list_of_files = []\n",
        "\n",
        "    for i in range(len(filenames)):\n",
        "      fi = \"/content/CSE508_Winter2023_Dataset/\"+filenames[i]\n",
        "      list_of_files.append(fi)\n",
        "    \n",
        "    return list_of_files"
      ],
      "metadata": {
        "id": "PmWxb84lTZtg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Functions"
      ],
      "metadata": {
        "id": "cFd_lFuZTvkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase(data):\n",
        "\n",
        "    # Parameters: data: type(string)\n",
        "    # returns: lowercase of data   \n",
        "     \n",
        "    return data.lower()"
      ],
      "metadata": {
        "id": "30q_9cXETrrr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_word_tokenize(corpus):\n",
        "  \n",
        "    # Parameters:corpus: type(string)   \n",
        "    # returns word-level tokenization of corpus\n",
        "\n",
        "    return word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "Xbku60vtVIyw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords_from_tokens(tokens, stopwords_set):\n",
        "  \n",
        "    # Parameters: tokens: type(list)\n",
        "    #             stopwords_set: type(set)\n",
        "    # returns: tokens without stopwords\n",
        "\n",
        "    tokens_sans_stopwords = [x for x in tokens if x not in stopwords_set] \n",
        "    return tokens_sans_stopwords"
      ],
      "metadata": {
        "id": "wv8KEcziTxQC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation_from_tokens(tokens):\n",
        "\n",
        "    # Parameters: tokens: type(list)\n",
        "    # returns: tokens without punctuation\n",
        "\n",
        "    tokens_sans_punctuation = [x.translate(str.maketrans('', '', string.punctuation)) for x in tokens]\n",
        "    return tokens_sans_punctuation"
      ],
      "metadata": {
        "id": "aWoN_okWT-IL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_blank_space_tokens(tokens):\n",
        "    \n",
        "    #Parameters: tokens: type(list)\n",
        "    #returns: tokens without blank tokens\n",
        "\n",
        "    tokens_sans_blank_space = [x for x in tokens if x!='']  \n",
        "    return tokens_sans_blank_space"
      ],
      "metadata": {
        "id": "im3CrcTeUHGE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus, stopwords_set):\n",
        "    # Convert the text to lower case\n",
        "    lowercase_corpus = lowercase(corpus)\n",
        "    #print(len(lowercase_corpus))\n",
        "    \n",
        "    # Perform word tokenization (word_tokenize also takes care of whitespace)\n",
        "    word_tokens = perform_word_tokenize(lowercase_corpus)\n",
        "    #print(len(word_tokens))\n",
        "    \n",
        "    # Remove stopwords from tokens\n",
        "    word_tokens_sans_stopwords = remove_stopwords_from_tokens(word_tokens, stopwords_set)\n",
        "    #print(len(word_tokens_sans_stopwords))\n",
        "    \n",
        "    # Remove punctuation marks from tokens\n",
        "    word_tokens_sans_punctuation = remove_punctuation_from_tokens(word_tokens_sans_stopwords)\n",
        "    #print(len(word_tokens_sans_punctuation))\n",
        "    \n",
        "    # Remove blank space tokens\n",
        "    word_tokens_sans_blank_tokens = remove_blank_space_tokens(word_tokens_sans_punctuation)\n",
        "    #print(len(word_tokens_sans_blank_tokens))\n",
        "    \n",
        "    # Stem tokens\n",
        "    #word_tokens_final = stemming(word_tokens_sans_blank_tokens)\n",
        "    \n",
        "    return word_tokens_sans_blank_tokens"
      ],
      "metadata": {
        "id": "XVFFgAnZUJEQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_documents(list_of_files, stopwords_set):\n",
        "    '''\n",
        "    Parameters:\n",
        "        list_of_files: type(list)\n",
        "        stopwords_set: type(set)\n",
        "    \n",
        "    returns: list of tokens obtained by preprocessing documents in all classes\n",
        "    '''\n",
        "    preprocessed_list_of_docs_tokens = []\n",
        "    for doc_path in list_of_files:\n",
        "        file = open(doc_path, 'r', encoding='utf-8', errors='ignore')\n",
        "        file_corpus = file.read()\n",
        "        file.close()\n",
        "        doc_tokens = preprocess(file_corpus, stopwords_set)\n",
        "        preprocessed_list_of_docs_tokens.append(doc_tokens)\n",
        "    \n",
        "    pi_file = open('Q1_tf_idf.pkl', 'wb')\n",
        "    pickle.dump(preprocessed_list_of_docs_tokens, pi_file)\n",
        "    pi_file.close()\n",
        "    \n",
        "    return preprocessed_list_of_docs_tokens"
      ],
      "metadata": {
        "id": "t3O2ET20cKOk"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_file_dictionary(list_of_files):\n",
        "    \n",
        "    # Paramteres: list_of_files: type(string)\n",
        "    # returns: file_dictionary with integer key and path_of_file as value\n",
        "  \n",
        "    file_dictionary = {}\n",
        "    for i in range(len(list_of_files)):\n",
        "        file_dictionary[i] = list_of_files[i]\n",
        "    \n",
        "    return file_dictionary"
      ],
      "metadata": {
        "id": "d3qCJQZVUMLA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. Data Preprocessing"
      ],
      "metadata": {
        "id": "SvAA72ftUTzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (i) Relevant Text Extraction"
      ],
      "metadata": {
        "id": "PUwrbpCMUXS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_root='/content/CSE508_Winter2023_Dataset/'\n",
        "corpus=PlaintextCorpusReader(corpus_root,'.*')"
      ],
      "metadata": {
        "id": "Hu4lZqDwURJ7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing names of all files\n",
        "\n",
        "filenames=corpus.fileids()\n",
        "print(filenames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0hsNxV1UZ3L",
        "outputId": "e6477a55-de6a-4427-8818-5c0569cdfd83"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cranfield0001', 'cranfield0002', 'cranfield0003', 'cranfield0004', 'cranfield0005', 'cranfield0006', 'cranfield0007', 'cranfield0008', 'cranfield0009', 'cranfield0010', 'cranfield0011', 'cranfield0012', 'cranfield0013', 'cranfield0014', 'cranfield0015', 'cranfield0016', 'cranfield0017', 'cranfield0018', 'cranfield0019', 'cranfield0020', 'cranfield0021', 'cranfield0022', 'cranfield0023', 'cranfield0024', 'cranfield0025', 'cranfield0026', 'cranfield0027', 'cranfield0028', 'cranfield0029', 'cranfield0030', 'cranfield0031', 'cranfield0032', 'cranfield0033', 'cranfield0034', 'cranfield0035', 'cranfield0036', 'cranfield0037', 'cranfield0038', 'cranfield0039', 'cranfield0040', 'cranfield0041', 'cranfield0042', 'cranfield0043', 'cranfield0044', 'cranfield0045', 'cranfield0046', 'cranfield0047', 'cranfield0048', 'cranfield0049', 'cranfield0050', 'cranfield0051', 'cranfield0052', 'cranfield0053', 'cranfield0054', 'cranfield0055', 'cranfield0056', 'cranfield0057', 'cranfield0058', 'cranfield0059', 'cranfield0060', 'cranfield0061', 'cranfield0062', 'cranfield0063', 'cranfield0064', 'cranfield0065', 'cranfield0066', 'cranfield0067', 'cranfield0068', 'cranfield0069', 'cranfield0070', 'cranfield0071', 'cranfield0072', 'cranfield0073', 'cranfield0074', 'cranfield0075', 'cranfield0076', 'cranfield0077', 'cranfield0078', 'cranfield0079', 'cranfield0080', 'cranfield0081', 'cranfield0082', 'cranfield0083', 'cranfield0084', 'cranfield0085', 'cranfield0086', 'cranfield0087', 'cranfield0088', 'cranfield0089', 'cranfield0090', 'cranfield0091', 'cranfield0092', 'cranfield0093', 'cranfield0094', 'cranfield0095', 'cranfield0096', 'cranfield0097', 'cranfield0098', 'cranfield0099', 'cranfield0100', 'cranfield0101', 'cranfield0102', 'cranfield0103', 'cranfield0104', 'cranfield0105', 'cranfield0106', 'cranfield0107', 'cranfield0108', 'cranfield0109', 'cranfield0110', 'cranfield0111', 'cranfield0112', 'cranfield0113', 'cranfield0114', 'cranfield0115', 'cranfield0116', 'cranfield0117', 'cranfield0118', 'cranfield0119', 'cranfield0120', 'cranfield0121', 'cranfield0122', 'cranfield0123', 'cranfield0124', 'cranfield0125', 'cranfield0126', 'cranfield0127', 'cranfield0128', 'cranfield0129', 'cranfield0130', 'cranfield0131', 'cranfield0132', 'cranfield0133', 'cranfield0134', 'cranfield0135', 'cranfield0136', 'cranfield0137', 'cranfield0138', 'cranfield0139', 'cranfield0140', 'cranfield0141', 'cranfield0142', 'cranfield0143', 'cranfield0144', 'cranfield0145', 'cranfield0146', 'cranfield0147', 'cranfield0148', 'cranfield0149', 'cranfield0150', 'cranfield0151', 'cranfield0152', 'cranfield0153', 'cranfield0154', 'cranfield0155', 'cranfield0156', 'cranfield0157', 'cranfield0158', 'cranfield0159', 'cranfield0160', 'cranfield0161', 'cranfield0162', 'cranfield0163', 'cranfield0164', 'cranfield0165', 'cranfield0166', 'cranfield0167', 'cranfield0168', 'cranfield0169', 'cranfield0170', 'cranfield0171', 'cranfield0172', 'cranfield0173', 'cranfield0174', 'cranfield0175', 'cranfield0176', 'cranfield0177', 'cranfield0178', 'cranfield0179', 'cranfield0180', 'cranfield0181', 'cranfield0182', 'cranfield0183', 'cranfield0184', 'cranfield0185', 'cranfield0186', 'cranfield0187', 'cranfield0188', 'cranfield0189', 'cranfield0190', 'cranfield0191', 'cranfield0192', 'cranfield0193', 'cranfield0194', 'cranfield0195', 'cranfield0196', 'cranfield0197', 'cranfield0198', 'cranfield0199', 'cranfield0200', 'cranfield0201', 'cranfield0202', 'cranfield0203', 'cranfield0204', 'cranfield0205', 'cranfield0206', 'cranfield0207', 'cranfield0208', 'cranfield0209', 'cranfield0210', 'cranfield0211', 'cranfield0212', 'cranfield0213', 'cranfield0214', 'cranfield0215', 'cranfield0216', 'cranfield0217', 'cranfield0218', 'cranfield0219', 'cranfield0220', 'cranfield0221', 'cranfield0222', 'cranfield0223', 'cranfield0224', 'cranfield0225', 'cranfield0226', 'cranfield0227', 'cranfield0228', 'cranfield0229', 'cranfield0230', 'cranfield0231', 'cranfield0232', 'cranfield0233', 'cranfield0234', 'cranfield0235', 'cranfield0236', 'cranfield0237', 'cranfield0238', 'cranfield0239', 'cranfield0240', 'cranfield0241', 'cranfield0242', 'cranfield0243', 'cranfield0244', 'cranfield0245', 'cranfield0246', 'cranfield0247', 'cranfield0248', 'cranfield0249', 'cranfield0250', 'cranfield0251', 'cranfield0252', 'cranfield0253', 'cranfield0254', 'cranfield0255', 'cranfield0256', 'cranfield0257', 'cranfield0258', 'cranfield0259', 'cranfield0260', 'cranfield0261', 'cranfield0262', 'cranfield0263', 'cranfield0264', 'cranfield0265', 'cranfield0266', 'cranfield0267', 'cranfield0268', 'cranfield0269', 'cranfield0270', 'cranfield0271', 'cranfield0272', 'cranfield0273', 'cranfield0274', 'cranfield0275', 'cranfield0276', 'cranfield0277', 'cranfield0278', 'cranfield0279', 'cranfield0280', 'cranfield0281', 'cranfield0282', 'cranfield0283', 'cranfield0284', 'cranfield0285', 'cranfield0286', 'cranfield0287', 'cranfield0288', 'cranfield0289', 'cranfield0290', 'cranfield0291', 'cranfield0292', 'cranfield0293', 'cranfield0294', 'cranfield0295', 'cranfield0296', 'cranfield0297', 'cranfield0298', 'cranfield0299', 'cranfield0300', 'cranfield0301', 'cranfield0302', 'cranfield0303', 'cranfield0304', 'cranfield0305', 'cranfield0306', 'cranfield0307', 'cranfield0308', 'cranfield0309', 'cranfield0310', 'cranfield0311', 'cranfield0312', 'cranfield0313', 'cranfield0314', 'cranfield0315', 'cranfield0316', 'cranfield0317', 'cranfield0318', 'cranfield0319', 'cranfield0320', 'cranfield0321', 'cranfield0322', 'cranfield0323', 'cranfield0324', 'cranfield0325', 'cranfield0326', 'cranfield0327', 'cranfield0328', 'cranfield0329', 'cranfield0330', 'cranfield0331', 'cranfield0332', 'cranfield0333', 'cranfield0334', 'cranfield0335', 'cranfield0336', 'cranfield0337', 'cranfield0338', 'cranfield0339', 'cranfield0340', 'cranfield0341', 'cranfield0342', 'cranfield0343', 'cranfield0344', 'cranfield0345', 'cranfield0346', 'cranfield0347', 'cranfield0348', 'cranfield0349', 'cranfield0350', 'cranfield0351', 'cranfield0352', 'cranfield0353', 'cranfield0354', 'cranfield0355', 'cranfield0356', 'cranfield0357', 'cranfield0358', 'cranfield0359', 'cranfield0360', 'cranfield0361', 'cranfield0362', 'cranfield0363', 'cranfield0364', 'cranfield0365', 'cranfield0366', 'cranfield0367', 'cranfield0368', 'cranfield0369', 'cranfield0370', 'cranfield0371', 'cranfield0372', 'cranfield0373', 'cranfield0374', 'cranfield0375', 'cranfield0376', 'cranfield0377', 'cranfield0378', 'cranfield0379', 'cranfield0380', 'cranfield0381', 'cranfield0382', 'cranfield0383', 'cranfield0384', 'cranfield0385', 'cranfield0386', 'cranfield0387', 'cranfield0388', 'cranfield0389', 'cranfield0390', 'cranfield0391', 'cranfield0392', 'cranfield0393', 'cranfield0394', 'cranfield0395', 'cranfield0396', 'cranfield0397', 'cranfield0398', 'cranfield0399', 'cranfield0400', 'cranfield0401', 'cranfield0402', 'cranfield0403', 'cranfield0404', 'cranfield0405', 'cranfield0406', 'cranfield0407', 'cranfield0408', 'cranfield0409', 'cranfield0410', 'cranfield0411', 'cranfield0412', 'cranfield0413', 'cranfield0414', 'cranfield0415', 'cranfield0416', 'cranfield0417', 'cranfield0418', 'cranfield0419', 'cranfield0420', 'cranfield0421', 'cranfield0422', 'cranfield0423', 'cranfield0424', 'cranfield0425', 'cranfield0426', 'cranfield0427', 'cranfield0428', 'cranfield0429', 'cranfield0430', 'cranfield0431', 'cranfield0432', 'cranfield0433', 'cranfield0434', 'cranfield0435', 'cranfield0436', 'cranfield0437', 'cranfield0438', 'cranfield0439', 'cranfield0440', 'cranfield0441', 'cranfield0442', 'cranfield0443', 'cranfield0444', 'cranfield0445', 'cranfield0446', 'cranfield0447', 'cranfield0448', 'cranfield0449', 'cranfield0450', 'cranfield0451', 'cranfield0452', 'cranfield0453', 'cranfield0454', 'cranfield0455', 'cranfield0456', 'cranfield0457', 'cranfield0458', 'cranfield0459', 'cranfield0460', 'cranfield0461', 'cranfield0462', 'cranfield0463', 'cranfield0464', 'cranfield0465', 'cranfield0466', 'cranfield0467', 'cranfield0468', 'cranfield0469', 'cranfield0470', 'cranfield0471', 'cranfield0472', 'cranfield0473', 'cranfield0474', 'cranfield0475', 'cranfield0476', 'cranfield0477', 'cranfield0478', 'cranfield0479', 'cranfield0480', 'cranfield0481', 'cranfield0482', 'cranfield0483', 'cranfield0484', 'cranfield0485', 'cranfield0486', 'cranfield0487', 'cranfield0488', 'cranfield0489', 'cranfield0490', 'cranfield0491', 'cranfield0492', 'cranfield0493', 'cranfield0494', 'cranfield0495', 'cranfield0496', 'cranfield0497', 'cranfield0498', 'cranfield0499', 'cranfield0500', 'cranfield0501', 'cranfield0502', 'cranfield0503', 'cranfield0504', 'cranfield0505', 'cranfield0506', 'cranfield0507', 'cranfield0508', 'cranfield0509', 'cranfield0510', 'cranfield0511', 'cranfield0512', 'cranfield0513', 'cranfield0514', 'cranfield0515', 'cranfield0516', 'cranfield0517', 'cranfield0518', 'cranfield0519', 'cranfield0520', 'cranfield0521', 'cranfield0522', 'cranfield0523', 'cranfield0524', 'cranfield0525', 'cranfield0526', 'cranfield0527', 'cranfield0528', 'cranfield0529', 'cranfield0530', 'cranfield0531', 'cranfield0532', 'cranfield0533', 'cranfield0534', 'cranfield0535', 'cranfield0536', 'cranfield0537', 'cranfield0538', 'cranfield0539', 'cranfield0540', 'cranfield0541', 'cranfield0542', 'cranfield0543', 'cranfield0544', 'cranfield0545', 'cranfield0546', 'cranfield0547', 'cranfield0548', 'cranfield0549', 'cranfield0550', 'cranfield0551', 'cranfield0552', 'cranfield0553', 'cranfield0554', 'cranfield0555', 'cranfield0556', 'cranfield0557', 'cranfield0558', 'cranfield0559', 'cranfield0560', 'cranfield0561', 'cranfield0562', 'cranfield0563', 'cranfield0564', 'cranfield0565', 'cranfield0566', 'cranfield0567', 'cranfield0568', 'cranfield0569', 'cranfield0570', 'cranfield0571', 'cranfield0572', 'cranfield0573', 'cranfield0574', 'cranfield0575', 'cranfield0576', 'cranfield0577', 'cranfield0578', 'cranfield0579', 'cranfield0580', 'cranfield0581', 'cranfield0582', 'cranfield0583', 'cranfield0584', 'cranfield0585', 'cranfield0586', 'cranfield0587', 'cranfield0588', 'cranfield0589', 'cranfield0590', 'cranfield0591', 'cranfield0592', 'cranfield0593', 'cranfield0594', 'cranfield0595', 'cranfield0596', 'cranfield0597', 'cranfield0598', 'cranfield0599', 'cranfield0600', 'cranfield0601', 'cranfield0602', 'cranfield0603', 'cranfield0604', 'cranfield0605', 'cranfield0606', 'cranfield0607', 'cranfield0608', 'cranfield0609', 'cranfield0610', 'cranfield0611', 'cranfield0612', 'cranfield0613', 'cranfield0614', 'cranfield0615', 'cranfield0616', 'cranfield0617', 'cranfield0618', 'cranfield0619', 'cranfield0620', 'cranfield0621', 'cranfield0622', 'cranfield0623', 'cranfield0624', 'cranfield0625', 'cranfield0626', 'cranfield0627', 'cranfield0628', 'cranfield0629', 'cranfield0630', 'cranfield0631', 'cranfield0632', 'cranfield0633', 'cranfield0634', 'cranfield0635', 'cranfield0636', 'cranfield0637', 'cranfield0638', 'cranfield0639', 'cranfield0640', 'cranfield0641', 'cranfield0642', 'cranfield0643', 'cranfield0644', 'cranfield0645', 'cranfield0646', 'cranfield0647', 'cranfield0648', 'cranfield0649', 'cranfield0650', 'cranfield0651', 'cranfield0652', 'cranfield0653', 'cranfield0654', 'cranfield0655', 'cranfield0656', 'cranfield0657', 'cranfield0658', 'cranfield0659', 'cranfield0660', 'cranfield0661', 'cranfield0662', 'cranfield0663', 'cranfield0664', 'cranfield0665', 'cranfield0666', 'cranfield0667', 'cranfield0668', 'cranfield0669', 'cranfield0670', 'cranfield0671', 'cranfield0672', 'cranfield0673', 'cranfield0674', 'cranfield0675', 'cranfield0676', 'cranfield0677', 'cranfield0678', 'cranfield0679', 'cranfield0680', 'cranfield0681', 'cranfield0682', 'cranfield0683', 'cranfield0684', 'cranfield0685', 'cranfield0686', 'cranfield0687', 'cranfield0688', 'cranfield0689', 'cranfield0690', 'cranfield0691', 'cranfield0692', 'cranfield0693', 'cranfield0694', 'cranfield0695', 'cranfield0696', 'cranfield0697', 'cranfield0698', 'cranfield0699', 'cranfield0700', 'cranfield0701', 'cranfield0702', 'cranfield0703', 'cranfield0704', 'cranfield0705', 'cranfield0706', 'cranfield0707', 'cranfield0708', 'cranfield0709', 'cranfield0710', 'cranfield0711', 'cranfield0712', 'cranfield0713', 'cranfield0714', 'cranfield0715', 'cranfield0716', 'cranfield0717', 'cranfield0718', 'cranfield0719', 'cranfield0720', 'cranfield0721', 'cranfield0722', 'cranfield0723', 'cranfield0724', 'cranfield0725', 'cranfield0726', 'cranfield0727', 'cranfield0728', 'cranfield0729', 'cranfield0730', 'cranfield0731', 'cranfield0732', 'cranfield0733', 'cranfield0734', 'cranfield0735', 'cranfield0736', 'cranfield0737', 'cranfield0738', 'cranfield0739', 'cranfield0740', 'cranfield0741', 'cranfield0742', 'cranfield0743', 'cranfield0744', 'cranfield0745', 'cranfield0746', 'cranfield0747', 'cranfield0748', 'cranfield0749', 'cranfield0750', 'cranfield0751', 'cranfield0752', 'cranfield0753', 'cranfield0754', 'cranfield0755', 'cranfield0756', 'cranfield0757', 'cranfield0758', 'cranfield0759', 'cranfield0760', 'cranfield0761', 'cranfield0762', 'cranfield0763', 'cranfield0764', 'cranfield0765', 'cranfield0766', 'cranfield0767', 'cranfield0768', 'cranfield0769', 'cranfield0770', 'cranfield0771', 'cranfield0772', 'cranfield0773', 'cranfield0774', 'cranfield0775', 'cranfield0776', 'cranfield0777', 'cranfield0778', 'cranfield0779', 'cranfield0780', 'cranfield0781', 'cranfield0782', 'cranfield0783', 'cranfield0784', 'cranfield0785', 'cranfield0786', 'cranfield0787', 'cranfield0788', 'cranfield0789', 'cranfield0790', 'cranfield0791', 'cranfield0792', 'cranfield0793', 'cranfield0794', 'cranfield0795', 'cranfield0796', 'cranfield0797', 'cranfield0798', 'cranfield0799', 'cranfield0800', 'cranfield0801', 'cranfield0802', 'cranfield0803', 'cranfield0804', 'cranfield0805', 'cranfield0806', 'cranfield0807', 'cranfield0808', 'cranfield0809', 'cranfield0810', 'cranfield0811', 'cranfield0812', 'cranfield0813', 'cranfield0814', 'cranfield0815', 'cranfield0816', 'cranfield0817', 'cranfield0818', 'cranfield0819', 'cranfield0820', 'cranfield0821', 'cranfield0822', 'cranfield0823', 'cranfield0824', 'cranfield0825', 'cranfield0826', 'cranfield0827', 'cranfield0828', 'cranfield0829', 'cranfield0830', 'cranfield0831', 'cranfield0832', 'cranfield0833', 'cranfield0834', 'cranfield0835', 'cranfield0836', 'cranfield0837', 'cranfield0838', 'cranfield0839', 'cranfield0840', 'cranfield0841', 'cranfield0842', 'cranfield0843', 'cranfield0844', 'cranfield0845', 'cranfield0846', 'cranfield0847', 'cranfield0848', 'cranfield0849', 'cranfield0850', 'cranfield0851', 'cranfield0852', 'cranfield0853', 'cranfield0854', 'cranfield0855', 'cranfield0856', 'cranfield0857', 'cranfield0858', 'cranfield0859', 'cranfield0860', 'cranfield0861', 'cranfield0862', 'cranfield0863', 'cranfield0864', 'cranfield0865', 'cranfield0866', 'cranfield0867', 'cranfield0868', 'cranfield0869', 'cranfield0870', 'cranfield0871', 'cranfield0872', 'cranfield0873', 'cranfield0874', 'cranfield0875', 'cranfield0876', 'cranfield0877', 'cranfield0878', 'cranfield0879', 'cranfield0880', 'cranfield0881', 'cranfield0882', 'cranfield0883', 'cranfield0884', 'cranfield0885', 'cranfield0886', 'cranfield0887', 'cranfield0888', 'cranfield0889', 'cranfield0890', 'cranfield0891', 'cranfield0892', 'cranfield0893', 'cranfield0894', 'cranfield0895', 'cranfield0896', 'cranfield0897', 'cranfield0898', 'cranfield0899', 'cranfield0900', 'cranfield0901', 'cranfield0902', 'cranfield0903', 'cranfield0904', 'cranfield0905', 'cranfield0906', 'cranfield0907', 'cranfield0908', 'cranfield0909', 'cranfield0910', 'cranfield0911', 'cranfield0912', 'cranfield0913', 'cranfield0914', 'cranfield0915', 'cranfield0916', 'cranfield0917', 'cranfield0918', 'cranfield0919', 'cranfield0920', 'cranfield0921', 'cranfield0922', 'cranfield0923', 'cranfield0924', 'cranfield0925', 'cranfield0926', 'cranfield0927', 'cranfield0928', 'cranfield0929', 'cranfield0930', 'cranfield0931', 'cranfield0932', 'cranfield0933', 'cranfield0934', 'cranfield0935', 'cranfield0936', 'cranfield0937', 'cranfield0938', 'cranfield0939', 'cranfield0940', 'cranfield0941', 'cranfield0942', 'cranfield0943', 'cranfield0944', 'cranfield0945', 'cranfield0946', 'cranfield0947', 'cranfield0948', 'cranfield0949', 'cranfield0950', 'cranfield0951', 'cranfield0952', 'cranfield0953', 'cranfield0954', 'cranfield0955', 'cranfield0956', 'cranfield0957', 'cranfield0958', 'cranfield0959', 'cranfield0960', 'cranfield0961', 'cranfield0962', 'cranfield0963', 'cranfield0964', 'cranfield0965', 'cranfield0966', 'cranfield0967', 'cranfield0968', 'cranfield0969', 'cranfield0970', 'cranfield0971', 'cranfield0972', 'cranfield0973', 'cranfield0974', 'cranfield0975', 'cranfield0976', 'cranfield0977', 'cranfield0978', 'cranfield0979', 'cranfield0980', 'cranfield0981', 'cranfield0982', 'cranfield0983', 'cranfield0984', 'cranfield0985', 'cranfield0986', 'cranfield0987', 'cranfield0988', 'cranfield0989', 'cranfield0990', 'cranfield0991', 'cranfield0992', 'cranfield0993', 'cranfield0994', 'cranfield0995', 'cranfield0996', 'cranfield0997', 'cranfield0998', 'cranfield0999', 'cranfield1000', 'cranfield1001', 'cranfield1002', 'cranfield1003', 'cranfield1004', 'cranfield1005', 'cranfield1006', 'cranfield1007', 'cranfield1008', 'cranfield1009', 'cranfield1010', 'cranfield1011', 'cranfield1012', 'cranfield1013', 'cranfield1014', 'cranfield1015', 'cranfield1016', 'cranfield1017', 'cranfield1018', 'cranfield1019', 'cranfield1020', 'cranfield1021', 'cranfield1022', 'cranfield1023', 'cranfield1024', 'cranfield1025', 'cranfield1026', 'cranfield1027', 'cranfield1028', 'cranfield1029', 'cranfield1030', 'cranfield1031', 'cranfield1032', 'cranfield1033', 'cranfield1034', 'cranfield1035', 'cranfield1036', 'cranfield1037', 'cranfield1038', 'cranfield1039', 'cranfield1040', 'cranfield1041', 'cranfield1042', 'cranfield1043', 'cranfield1044', 'cranfield1045', 'cranfield1046', 'cranfield1047', 'cranfield1048', 'cranfield1049', 'cranfield1050', 'cranfield1051', 'cranfield1052', 'cranfield1053', 'cranfield1054', 'cranfield1055', 'cranfield1056', 'cranfield1057', 'cranfield1058', 'cranfield1059', 'cranfield1060', 'cranfield1061', 'cranfield1062', 'cranfield1063', 'cranfield1064', 'cranfield1065', 'cranfield1066', 'cranfield1067', 'cranfield1068', 'cranfield1069', 'cranfield1070', 'cranfield1071', 'cranfield1072', 'cranfield1073', 'cranfield1074', 'cranfield1075', 'cranfield1076', 'cranfield1077', 'cranfield1078', 'cranfield1079', 'cranfield1080', 'cranfield1081', 'cranfield1082', 'cranfield1083', 'cranfield1084', 'cranfield1085', 'cranfield1086', 'cranfield1087', 'cranfield1088', 'cranfield1089', 'cranfield1090', 'cranfield1091', 'cranfield1092', 'cranfield1093', 'cranfield1094', 'cranfield1095', 'cranfield1096', 'cranfield1097', 'cranfield1098', 'cranfield1099', 'cranfield1100', 'cranfield1101', 'cranfield1102', 'cranfield1103', 'cranfield1104', 'cranfield1105', 'cranfield1106', 'cranfield1107', 'cranfield1108', 'cranfield1109', 'cranfield1110', 'cranfield1111', 'cranfield1112', 'cranfield1113', 'cranfield1114', 'cranfield1115', 'cranfield1116', 'cranfield1117', 'cranfield1118', 'cranfield1119', 'cranfield1120', 'cranfield1121', 'cranfield1122', 'cranfield1123', 'cranfield1124', 'cranfield1125', 'cranfield1126', 'cranfield1127', 'cranfield1128', 'cranfield1129', 'cranfield1130', 'cranfield1131', 'cranfield1132', 'cranfield1133', 'cranfield1134', 'cranfield1135', 'cranfield1136', 'cranfield1137', 'cranfield1138', 'cranfield1139', 'cranfield1140', 'cranfield1141', 'cranfield1142', 'cranfield1143', 'cranfield1144', 'cranfield1145', 'cranfield1146', 'cranfield1147', 'cranfield1148', 'cranfield1149', 'cranfield1150', 'cranfield1151', 'cranfield1152', 'cranfield1153', 'cranfield1154', 'cranfield1155', 'cranfield1156', 'cranfield1157', 'cranfield1158', 'cranfield1159', 'cranfield1160', 'cranfield1161', 'cranfield1162', 'cranfield1163', 'cranfield1164', 'cranfield1165', 'cranfield1166', 'cranfield1167', 'cranfield1168', 'cranfield1169', 'cranfield1170', 'cranfield1171', 'cranfield1172', 'cranfield1173', 'cranfield1174', 'cranfield1175', 'cranfield1176', 'cranfield1177', 'cranfield1178', 'cranfield1179', 'cranfield1180', 'cranfield1181', 'cranfield1182', 'cranfield1183', 'cranfield1184', 'cranfield1185', 'cranfield1186', 'cranfield1187', 'cranfield1188', 'cranfield1189', 'cranfield1190', 'cranfield1191', 'cranfield1192', 'cranfield1193', 'cranfield1194', 'cranfield1195', 'cranfield1196', 'cranfield1197', 'cranfield1198', 'cranfield1199', 'cranfield1200', 'cranfield1201', 'cranfield1202', 'cranfield1203', 'cranfield1204', 'cranfield1205', 'cranfield1206', 'cranfield1207', 'cranfield1208', 'cranfield1209', 'cranfield1210', 'cranfield1211', 'cranfield1212', 'cranfield1213', 'cranfield1214', 'cranfield1215', 'cranfield1216', 'cranfield1217', 'cranfield1218', 'cranfield1219', 'cranfield1220', 'cranfield1221', 'cranfield1222', 'cranfield1223', 'cranfield1224', 'cranfield1225', 'cranfield1226', 'cranfield1227', 'cranfield1228', 'cranfield1229', 'cranfield1230', 'cranfield1231', 'cranfield1232', 'cranfield1233', 'cranfield1234', 'cranfield1235', 'cranfield1236', 'cranfield1237', 'cranfield1238', 'cranfield1239', 'cranfield1240', 'cranfield1241', 'cranfield1242', 'cranfield1243', 'cranfield1244', 'cranfield1245', 'cranfield1246', 'cranfield1247', 'cranfield1248', 'cranfield1249', 'cranfield1250', 'cranfield1251', 'cranfield1252', 'cranfield1253', 'cranfield1254', 'cranfield1255', 'cranfield1256', 'cranfield1257', 'cranfield1258', 'cranfield1259', 'cranfield1260', 'cranfield1261', 'cranfield1262', 'cranfield1263', 'cranfield1264', 'cranfield1265', 'cranfield1266', 'cranfield1267', 'cranfield1268', 'cranfield1269', 'cranfield1270', 'cranfield1271', 'cranfield1272', 'cranfield1273', 'cranfield1274', 'cranfield1275', 'cranfield1276', 'cranfield1277', 'cranfield1278', 'cranfield1279', 'cranfield1280', 'cranfield1281', 'cranfield1282', 'cranfield1283', 'cranfield1284', 'cranfield1285', 'cranfield1286', 'cranfield1287', 'cranfield1288', 'cranfield1289', 'cranfield1290', 'cranfield1291', 'cranfield1292', 'cranfield1293', 'cranfield1294', 'cranfield1295', 'cranfield1296', 'cranfield1297', 'cranfield1298', 'cranfield1299', 'cranfield1300', 'cranfield1301', 'cranfield1302', 'cranfield1303', 'cranfield1304', 'cranfield1305', 'cranfield1306', 'cranfield1307', 'cranfield1308', 'cranfield1309', 'cranfield1310', 'cranfield1311', 'cranfield1312', 'cranfield1313', 'cranfield1314', 'cranfield1315', 'cranfield1316', 'cranfield1317', 'cranfield1318', 'cranfield1319', 'cranfield1320', 'cranfield1321', 'cranfield1322', 'cranfield1323', 'cranfield1324', 'cranfield1325', 'cranfield1326', 'cranfield1327', 'cranfield1328', 'cranfield1329', 'cranfield1330', 'cranfield1331', 'cranfield1332', 'cranfield1333', 'cranfield1334', 'cranfield1335', 'cranfield1336', 'cranfield1337', 'cranfield1338', 'cranfield1339', 'cranfield1340', 'cranfield1341', 'cranfield1342', 'cranfield1343', 'cranfield1344', 'cranfield1345', 'cranfield1346', 'cranfield1347', 'cranfield1348', 'cranfield1349', 'cranfield1350', 'cranfield1351', 'cranfield1352', 'cranfield1353', 'cranfield1354', 'cranfield1355', 'cranfield1356', 'cranfield1357', 'cranfield1358', 'cranfield1359', 'cranfield1360', 'cranfield1361', 'cranfield1362', 'cranfield1363', 'cranfield1364', 'cranfield1365', 'cranfield1366', 'cranfield1367', 'cranfield1368', 'cranfield1369', 'cranfield1370', 'cranfield1371', 'cranfield1372', 'cranfield1373', 'cranfield1374', 'cranfield1375', 'cranfield1376', 'cranfield1377', 'cranfield1378', 'cranfield1379', 'cranfield1380', 'cranfield1381', 'cranfield1382', 'cranfield1383', 'cranfield1384', 'cranfield1385', 'cranfield1386', 'cranfield1387', 'cranfield1388', 'cranfield1389', 'cranfield1390', 'cranfield1391', 'cranfield1392', 'cranfield1393', 'cranfield1394', 'cranfield1395', 'cranfield1396', 'cranfield1397', 'cranfield1398', 'cranfield1399', 'cranfield1400']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(filenames))\n",
        "print(len(filenames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH1WlCchUb76",
        "outputId": "1d5e25c1-5af9-448e-bbb5-00d97f6ff48d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "1400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First 5 Documents"
      ],
      "metadata": {
        "id": "pby700qfUgHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing contents of first 5 files before extracting\n",
        "\n",
        "for i in range(0, 5):\n",
        "  outfile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "  doc = outfile.read()\n",
        "  print(\"------------\")\n",
        "  print(\"Document: \"+str(i+1))\n",
        "  print(\"------------\")\n",
        "  print(doc)\n",
        "  print('-----------------------------------------------------------------')\n",
        "  print('')\n",
        "  outfile.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkLMEk3FUeCG",
        "outputId": "1f1c827c-b841-4847-c8e3-0373b77a083a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------\n",
            "Document: 1\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "1\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "experimental investigation of the aerodynamics of a\n",
            "wing in a slipstream .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "brenckman,m.\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "j. ae. scs. 25, 1958, 324.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "  an experimental study of a wing in a propeller slipstream was\n",
            "made in order to determine the spanwise distribution of the lift\n",
            "increase due to slipstream at different angles of attack of the wing\n",
            "and at different free stream to slipstream velocity ratios .  the\n",
            "results were intended in part as an evaluation basis for different\n",
            "theoretical treatments of this problem .\n",
            "  the comparative span loading curves, together with supporting\n",
            "evidence, showed that a substantial part of the lift increment\n",
            "produced by the slipstream was due to a /destalling/ or boundary-layer-control\n",
            "effect .  the integrated remaining lift increment,\n",
            "after subtracting this destalling lift, was found to agree\n",
            "well with a potential flow theory .\n",
            "  an empirical evaluation of the destalling effects was made for\n",
            "the specific configuration of the experiment .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 2\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "2\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "simple shear flow past a flat plate in an incompressible fluid of small\n",
            "viscosity .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "ting-yili\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "department of aeronautical engineering, rensselaer polytechnic\n",
            "institute\n",
            "troy, n.y.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "in the study of high-speed viscous flow past a two-dimensional body it\n",
            "is usually necessary to consider a curved shock wave emitting from the\n",
            "nose or leading edge of the body .  consequently, there exists an inviscid\n",
            "rotational flow region between the shock wave and the boundary layer\n",
            ".  such a situation arises, for instance, in the study of the hypersonic\n",
            "viscous flow past a flat plate .  the situation is somewhat different\n",
            "from prandtl's classical boundary-layer problem . in prandtl's\n",
            "original problem the inviscid free stream outside the boundary layer is\n",
            "irrotational while in a hypersonic boundary-layer problem the inviscid\n",
            "free stream must be considered as rotational .  the possible effects of\n",
            "vorticity have been recently discussed by ferri and libby .  in the present\n",
            "paper, the simple shear flow past a flat plate in a fluid of small\n",
            "viscosity is investigated .  it can be shown that this problem can again\n",
            "be treated by the boundary-layer approximation, the only novel feature\n",
            "being that the free stream has a constant vorticity .  the discussion\n",
            "here is restricted to two-dimensional incompressible steady flow .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 3\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "3\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "the boundary layer in simple shear flow past a flat plate .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "m. b. glauert\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "department of mathematics, university of manchester, manchester,\n",
            "england\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "the boundary-layer equations are presented for steady\n",
            "incompressible flow with no pressure gradient .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 4\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "4\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "approximate solutions of the incompressible laminar\n",
            "boundary layer equations for a plate in shear flow .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "yen,k.t.\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "j. ae. scs. 22, 1955, 728.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "  the two-dimensional steady boundary-layer\n",
            "problem for a flat plate in a\n",
            "shear flow of incompressible fluid is considered .\n",
            "solutions for the boundarylayer\n",
            "thickness, skin friction, and the velocity\n",
            "distribution in the boundary\n",
            "layer are obtained by the karman-pohlhausen\n",
            "technique .  comparison with\n",
            "the boundary layer of a uniform flow has also\n",
            "been made to show the effect of\n",
            "vorticity .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 5\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "5\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "one-dimensional transient heat conduction into a double-layer\n",
            "slab subjected to a linear heat input for a small time\n",
            "internal .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "wasserman,b.\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "j. ae. scs. 24, 1957, 924.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "  analytic solutions are presented for the transient heat conduction\n",
            "in composite slabs exposed at one surface to a\n",
            "triangular heat rate .  this type of heating rate may occur, for\n",
            "example, during aerodynamic heating .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting content between TITLE and TEXT tag from all documents."
      ],
      "metadata": {
        "id": "yQxD-lsWUo-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(filenames)):\n",
        "    outfile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "    doc = outfile.read()\n",
        "\n",
        "    # initializing string\n",
        "    test_str = doc\n",
        "\n",
        "    # initializing tags\n",
        "    tag1 = \"TITLE\"\n",
        "    tag2 = \"TEXT\"\n",
        "\n",
        "    # regex to extract required strings\n",
        "    reg_str1 = \"<\"+tag1+\">(.*?)</\"+tag1+\">\"\n",
        "    res1 = re.findall(reg_str1, test_str, re.DOTALL)\n",
        "\n",
        "    reg_str2 = \"<\"+tag2+\">(.*?)</\"+tag2+\">\"\n",
        "    res2 = re.findall(reg_str2, test_str, re.DOTALL)\n",
        "\n",
        "    #Combining contents of TITLE and TEXT\n",
        "    res = res1+res2\n",
        "    \n",
        "    s = res\n",
        "\n",
        "    # using list comprehension\n",
        "    listToStr = ' '.join([str(elem) for elem in s])\n",
        "\n",
        "    writeFile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'w')\n",
        "    L = listToStr\n",
        "\n",
        "    writeFile.write(L)\n",
        "    writeFile.close()\n",
        "\n",
        "    outfile.close()"
      ],
      "metadata": {
        "id": "Lm6oSqqjUiLo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First 5 Documents after extracting contents between TITLE and TEXT tag."
      ],
      "metadata": {
        "id": "WBuRQj9QUuvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 5):\n",
        "  outfile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "  doc = outfile.read()\n",
        "  print(\"------------\")\n",
        "  print(\"Document: \"+str(i+1))\n",
        "  print(\"------------\")\n",
        "  print(doc)\n",
        "  print('-----------------------------------------------------------------')\n",
        "  print('')\n",
        "  outfile.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjaG6ro6Ur2p",
        "outputId": "38b0a00a-2367-4403-d79a-4de2fb450bd8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------\n",
            "Document: 1\n",
            "------------\n",
            "\n",
            "experimental investigation of the aerodynamics of a\n",
            "wing in a slipstream .\n",
            " \n",
            "  an experimental study of a wing in a propeller slipstream was\n",
            "made in order to determine the spanwise distribution of the lift\n",
            "increase due to slipstream at different angles of attack of the wing\n",
            "and at different free stream to slipstream velocity ratios .  the\n",
            "results were intended in part as an evaluation basis for different\n",
            "theoretical treatments of this problem .\n",
            "  the comparative span loading curves, together with supporting\n",
            "evidence, showed that a substantial part of the lift increment\n",
            "produced by the slipstream was due to a /destalling/ or boundary-layer-control\n",
            "effect .  the integrated remaining lift increment,\n",
            "after subtracting this destalling lift, was found to agree\n",
            "well with a potential flow theory .\n",
            "  an empirical evaluation of the destalling effects was made for\n",
            "the specific configuration of the experiment .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 2\n",
            "------------\n",
            "\n",
            "simple shear flow past a flat plate in an incompressible fluid of small\n",
            "viscosity .\n",
            " \n",
            "in the study of high-speed viscous flow past a two-dimensional body it\n",
            "is usually necessary to consider a curved shock wave emitting from the\n",
            "nose or leading edge of the body .  consequently, there exists an inviscid\n",
            "rotational flow region between the shock wave and the boundary layer\n",
            ".  such a situation arises, for instance, in the study of the hypersonic\n",
            "viscous flow past a flat plate .  the situation is somewhat different\n",
            "from prandtl's classical boundary-layer problem . in prandtl's\n",
            "original problem the inviscid free stream outside the boundary layer is\n",
            "irrotational while in a hypersonic boundary-layer problem the inviscid\n",
            "free stream must be considered as rotational .  the possible effects of\n",
            "vorticity have been recently discussed by ferri and libby .  in the present\n",
            "paper, the simple shear flow past a flat plate in a fluid of small\n",
            "viscosity is investigated .  it can be shown that this problem can again\n",
            "be treated by the boundary-layer approximation, the only novel feature\n",
            "being that the free stream has a constant vorticity .  the discussion\n",
            "here is restricted to two-dimensional incompressible steady flow .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 3\n",
            "------------\n",
            "\n",
            "the boundary layer in simple shear flow past a flat plate .\n",
            " \n",
            "the boundary-layer equations are presented for steady\n",
            "incompressible flow with no pressure gradient .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 4\n",
            "------------\n",
            "\n",
            "approximate solutions of the incompressible laminar\n",
            "boundary layer equations for a plate in shear flow .\n",
            " \n",
            "  the two-dimensional steady boundary-layer\n",
            "problem for a flat plate in a\n",
            "shear flow of incompressible fluid is considered .\n",
            "solutions for the boundarylayer\n",
            "thickness, skin friction, and the velocity\n",
            "distribution in the boundary\n",
            "layer are obtained by the karman-pohlhausen\n",
            "technique .  comparison with\n",
            "the boundary layer of a uniform flow has also\n",
            "been made to show the effect of\n",
            "vorticity .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 5\n",
            "------------\n",
            "\n",
            "one-dimensional transient heat conduction into a double-layer\n",
            "slab subjected to a linear heat input for a small time\n",
            "internal .\n",
            " \n",
            "  analytic solutions are presented for the transient heat conduction\n",
            "in composite slabs exposed at one surface to a\n",
            "triangular heat rate .  this type of heating rate may occur, for\n",
            "example, during aerodynamic heating .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (ii) Preprocessing"
      ],
      "metadata": {
        "id": "7nFfu7j0Uzib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Lowercase the text"
      ],
      "metadata": {
        "id": "GTETLsLRU1QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newfiles=[]\n",
        "for i in range(len(filenames)):\n",
        "  fileind=open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "  filedata=fileind.read()\n",
        "  filedata=lowercase(filedata)\n",
        "  newfiles.append(filedata)\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m74_zIXxUwP1",
        "outputId": "ac536822-9a33-493a-a8cc-d29049b645c4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "\n",
            "experimental investigation of the aerodynamics of a\n",
            "wing in a slipstream .\n",
            " \n",
            "  an experimental study of a wing in a propeller slipstream was\n",
            "made in order to determine the spanwise distribution of the lift\n",
            "increase due to slipstream at different angles of attack of the wing\n",
            "and at different free stream to slipstream velocity ratios .  the\n",
            "results were intended in part as an evaluation basis for different\n",
            "theoretical treatments of this problem .\n",
            "  the comparative span loading curves, together with supporting\n",
            "evidence, showed that a substantial part of the lift increment\n",
            "produced by the slipstream was due to a /destalling/ or boundary-layer-control\n",
            "effect .  the integrated remaining lift increment,\n",
            "after subtracting this destalling lift, was found to agree\n",
            "well with a potential flow theory .\n",
            "  an empirical evaluation of the destalling effects was made for\n",
            "the specific configuration of the experiment .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "\n",
            "simple shear flow past a flat plate in an incompressible fluid of small\n",
            "viscosity .\n",
            " \n",
            "in the study of high-speed viscous flow past a two-dimensional body it\n",
            "is usually necessary to consider a curved shock wave emitting from the\n",
            "nose or leading edge of the body .  consequently, there exists an inviscid\n",
            "rotational flow region between the shock wave and the boundary layer\n",
            ".  such a situation arises, for instance, in the study of the hypersonic\n",
            "viscous flow past a flat plate .  the situation is somewhat different\n",
            "from prandtl's classical boundary-layer problem . in prandtl's\n",
            "original problem the inviscid free stream outside the boundary layer is\n",
            "irrotational while in a hypersonic boundary-layer problem the inviscid\n",
            "free stream must be considered as rotational .  the possible effects of\n",
            "vorticity have been recently discussed by ferri and libby .  in the present\n",
            "paper, the simple shear flow past a flat plate in a fluid of small\n",
            "viscosity is investigated .  it can be shown that this problem can again\n",
            "be treated by the boundary-layer approximation, the only novel feature\n",
            "being that the free stream has a constant vorticity .  the discussion\n",
            "here is restricted to two-dimensional incompressible steady flow .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "\n",
            "the boundary layer in simple shear flow past a flat plate .\n",
            " \n",
            "the boundary-layer equations are presented for steady\n",
            "incompressible flow with no pressure gradient .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "\n",
            "approximate solutions of the incompressible laminar\n",
            "boundary layer equations for a plate in shear flow .\n",
            " \n",
            "  the two-dimensional steady boundary-layer\n",
            "problem for a flat plate in a\n",
            "shear flow of incompressible fluid is considered .\n",
            "solutions for the boundarylayer\n",
            "thickness, skin friction, and the velocity\n",
            "distribution in the boundary\n",
            "layer are obtained by the karman-pohlhausen\n",
            "technique .  comparison with\n",
            "the boundary layer of a uniform flow has also\n",
            "been made to show the effect of\n",
            "vorticity .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "\n",
            "one-dimensional transient heat conduction into a double-layer\n",
            "slab subjected to a linear heat input for a small time\n",
            "internal .\n",
            " \n",
            "  analytic solutions are presented for the transient heat conduction\n",
            "in composite slabs exposed at one surface to a\n",
            "triangular heat rate .  this type of heating rate may occur, for\n",
            "example, during aerodynamic heating .\n",
            "\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Perform tokenization"
      ],
      "metadata": {
        "id": "Ayl0Ud2rU8RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  filedata=perform_word_tokenize(filedata)\n",
        "  newfiles[i] = filedata\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-gARkTaU5ZU",
        "outputId": "87444ae2-dd7d-441e-da60-829329b73cba"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'of', 'the', 'aerodynamics', 'of', 'a', 'wing', 'in', 'a', 'slipstream', '.', 'an', 'experimental', 'study', 'of', 'a', 'wing', 'in', 'a', 'propeller', 'slipstream', 'was', 'made', 'in', 'order', 'to', 'determine', 'the', 'spanwise', 'distribution', 'of', 'the', 'lift', 'increase', 'due', 'to', 'slipstream', 'at', 'different', 'angles', 'of', 'attack', 'of', 'the', 'wing', 'and', 'at', 'different', 'free', 'stream', 'to', 'slipstream', 'velocity', 'ratios', '.', 'the', 'results', 'were', 'intended', 'in', 'part', 'as', 'an', 'evaluation', 'basis', 'for', 'different', 'theoretical', 'treatments', 'of', 'this', 'problem', '.', 'the', 'comparative', 'span', 'loading', 'curves', ',', 'together', 'with', 'supporting', 'evidence', ',', 'showed', 'that', 'a', 'substantial', 'part', 'of', 'the', 'lift', 'increment', 'produced', 'by', 'the', 'slipstream', 'was', 'due', 'to', 'a', '/destalling/', 'or', 'boundary-layer-control', 'effect', '.', 'the', 'integrated', 'remaining', 'lift', 'increment', ',', 'after', 'subtracting', 'this', 'destalling', 'lift', ',', 'was', 'found', 'to', 'agree', 'well', 'with', 'a', 'potential', 'flow', 'theory', '.', 'an', 'empirical', 'evaluation', 'of', 'the', 'destalling', 'effects', 'was', 'made', 'for', 'the', 'specific', 'configuration', 'of', 'the', 'experiment', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'a', 'flat', 'plate', 'in', 'an', 'incompressible', 'fluid', 'of', 'small', 'viscosity', '.', 'in', 'the', 'study', 'of', 'high-speed', 'viscous', 'flow', 'past', 'a', 'two-dimensional', 'body', 'it', 'is', 'usually', 'necessary', 'to', 'consider', 'a', 'curved', 'shock', 'wave', 'emitting', 'from', 'the', 'nose', 'or', 'leading', 'edge', 'of', 'the', 'body', '.', 'consequently', ',', 'there', 'exists', 'an', 'inviscid', 'rotational', 'flow', 'region', 'between', 'the', 'shock', 'wave', 'and', 'the', 'boundary', 'layer', '.', 'such', 'a', 'situation', 'arises', ',', 'for', 'instance', ',', 'in', 'the', 'study', 'of', 'the', 'hypersonic', 'viscous', 'flow', 'past', 'a', 'flat', 'plate', '.', 'the', 'situation', 'is', 'somewhat', 'different', 'from', 'prandtl', \"'s\", 'classical', 'boundary-layer', 'problem', '.', 'in', \"prandtl's\", 'original', 'problem', 'the', 'inviscid', 'free', 'stream', 'outside', 'the', 'boundary', 'layer', 'is', 'irrotational', 'while', 'in', 'a', 'hypersonic', 'boundary-layer', 'problem', 'the', 'inviscid', 'free', 'stream', 'must', 'be', 'considered', 'as', 'rotational', '.', 'the', 'possible', 'effects', 'of', 'vorticity', 'have', 'been', 'recently', 'discussed', 'by', 'ferri', 'and', 'libby', '.', 'in', 'the', 'present', 'paper', ',', 'the', 'simple', 'shear', 'flow', 'past', 'a', 'flat', 'plate', 'in', 'a', 'fluid', 'of', 'small', 'viscosity', 'is', 'investigated', '.', 'it', 'can', 'be', 'shown', 'that', 'this', 'problem', 'can', 'again', 'be', 'treated', 'by', 'the', 'boundary-layer', 'approximation', ',', 'the', 'only', 'novel', 'feature', 'being', 'that', 'the', 'free', 'stream', 'has', 'a', 'constant', 'vorticity', '.', 'the', 'discussion', 'here', 'is', 'restricted', 'to', 'two-dimensional', 'incompressible', 'steady', 'flow', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['the', 'boundary', 'layer', 'in', 'simple', 'shear', 'flow', 'past', 'a', 'flat', 'plate', '.', 'the', 'boundary-layer', 'equations', 'are', 'presented', 'for', 'steady', 'incompressible', 'flow', 'with', 'no', 'pressure', 'gradient', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'of', 'the', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'for', 'a', 'plate', 'in', 'shear', 'flow', '.', 'the', 'two-dimensional', 'steady', 'boundary-layer', 'problem', 'for', 'a', 'flat', 'plate', 'in', 'a', 'shear', 'flow', 'of', 'incompressible', 'fluid', 'is', 'considered', '.', 'solutions', 'for', 'the', 'boundarylayer', 'thickness', ',', 'skin', 'friction', ',', 'and', 'the', 'velocity', 'distribution', 'in', 'the', 'boundary', 'layer', 'are', 'obtained', 'by', 'the', 'karman-pohlhausen', 'technique', '.', 'comparison', 'with', 'the', 'boundary', 'layer', 'of', 'a', 'uniform', 'flow', 'has', 'also', 'been', 'made', 'to', 'show', 'the', 'effect', 'of', 'vorticity', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['one-dimensional', 'transient', 'heat', 'conduction', 'into', 'a', 'double-layer', 'slab', 'subjected', 'to', 'a', 'linear', 'heat', 'input', 'for', 'a', 'small', 'time', 'internal', '.', 'analytic', 'solutions', 'are', 'presented', 'for', 'the', 'transient', 'heat', 'conduction', 'in', 'composite', 'slabs', 'exposed', 'at', 'one', 'surface', 'to', 'a', 'triangular', 'heat', 'rate', '.', 'this', 'type', 'of', 'heating', 'rate', 'may', 'occur', ',', 'for', 'example', ',', 'during', 'aerodynamic', 'heating', '.']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_set = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "DYuFaUWpU96X"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Remove stopwords"
      ],
      "metadata": {
        "id": "Wy1mdSR-VVjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  tokens_sans_stopwords = remove_stopwords_from_tokens(filedata, stopwords_set)\n",
        "  newfiles[i] = tokens_sans_stopwords\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX7AzcaMVTlC",
        "outputId": "70e302db-9103-4209-8133-cc518f6b1adc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', '.', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution', 'lift', 'increase', 'due', 'slipstream', 'different', 'angles', 'attack', 'wing', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', '.', 'results', 'intended', 'part', 'evaluation', 'basis', 'different', 'theoretical', 'treatments', 'problem', '.', 'comparative', 'span', 'loading', 'curves', ',', 'together', 'supporting', 'evidence', ',', 'showed', 'substantial', 'part', 'lift', 'increment', 'produced', 'slipstream', 'due', '/destalling/', 'boundary-layer-control', 'effect', '.', 'integrated', 'remaining', 'lift', 'increment', ',', 'subtracting', 'destalling', 'lift', ',', 'found', 'agree', 'well', 'potential', 'flow', 'theory', '.', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'specific', 'configuration', 'experiment', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', '.', 'study', 'high-speed', 'viscous', 'flow', 'past', 'two-dimensional', 'body', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'nose', 'leading', 'edge', 'body', '.', 'consequently', ',', 'exists', 'inviscid', 'rotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', '.', 'situation', 'arises', ',', 'instance', ',', 'study', 'hypersonic', 'viscous', 'flow', 'past', 'flat', 'plate', '.', 'situation', 'somewhat', 'different', 'prandtl', \"'s\", 'classical', 'boundary-layer', 'problem', '.', \"prandtl's\", 'original', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'irrotational', 'hypersonic', 'boundary-layer', 'problem', 'inviscid', 'free', 'stream', 'must', 'considered', 'rotational', '.', 'possible', 'effects', 'vorticity', 'recently', 'discussed', 'ferri', 'libby', '.', 'present', 'paper', ',', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'small', 'viscosity', 'investigated', '.', 'shown', 'problem', 'treated', 'boundary-layer', 'approximation', ',', 'novel', 'feature', 'free', 'stream', 'constant', 'vorticity', '.', 'discussion', 'restricted', 'two-dimensional', 'incompressible', 'steady', 'flow', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['boundary', 'layer', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', '.', 'boundary-layer', 'equations', 'presented', 'steady', 'incompressible', 'flow', 'pressure', 'gradient', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'plate', 'shear', 'flow', '.', 'two-dimensional', 'steady', 'boundary-layer', 'problem', 'flat', 'plate', 'shear', 'flow', 'incompressible', 'fluid', 'considered', '.', 'solutions', 'boundarylayer', 'thickness', ',', 'skin', 'friction', ',', 'velocity', 'distribution', 'boundary', 'layer', 'obtained', 'karman-pohlhausen', 'technique', '.', 'comparison', 'boundary', 'layer', 'uniform', 'flow', 'also', 'made', 'show', 'effect', 'vorticity', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['one-dimensional', 'transient', 'heat', 'conduction', 'double-layer', 'slab', 'subjected', 'linear', 'heat', 'input', 'small', 'time', 'internal', '.', 'analytic', 'solutions', 'presented', 'transient', 'heat', 'conduction', 'composite', 'slabs', 'exposed', 'one', 'surface', 'triangular', 'heat', 'rate', '.', 'type', 'heating', 'rate', 'may', 'occur', ',', 'example', ',', 'aerodynamic', 'heating', '.']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Remove punctuations"
      ],
      "metadata": {
        "id": "5aW5_CJaVdvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  tokens_sans_punctuation = remove_punctuation_from_tokens(filedata)\n",
        "  newfiles[i] = tokens_sans_punctuation\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkVA-WvnVbAe",
        "outputId": "620e9e59-dab7-4103-ea5b-5802b12bf685"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', '', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution', 'lift', 'increase', 'due', 'slipstream', 'different', 'angles', 'attack', 'wing', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', '', 'results', 'intended', 'part', 'evaluation', 'basis', 'different', 'theoretical', 'treatments', 'problem', '', 'comparative', 'span', 'loading', 'curves', '', 'together', 'supporting', 'evidence', '', 'showed', 'substantial', 'part', 'lift', 'increment', 'produced', 'slipstream', 'due', 'destalling', 'boundarylayercontrol', 'effect', '', 'integrated', 'remaining', 'lift', 'increment', '', 'subtracting', 'destalling', 'lift', '', 'found', 'agree', 'well', 'potential', 'flow', 'theory', '', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'specific', 'configuration', 'experiment', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', '', 'study', 'highspeed', 'viscous', 'flow', 'past', 'twodimensional', 'body', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'nose', 'leading', 'edge', 'body', '', 'consequently', '', 'exists', 'inviscid', 'rotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', '', 'situation', 'arises', '', 'instance', '', 'study', 'hypersonic', 'viscous', 'flow', 'past', 'flat', 'plate', '', 'situation', 'somewhat', 'different', 'prandtl', 's', 'classical', 'boundarylayer', 'problem', '', 'prandtls', 'original', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'irrotational', 'hypersonic', 'boundarylayer', 'problem', 'inviscid', 'free', 'stream', 'must', 'considered', 'rotational', '', 'possible', 'effects', 'vorticity', 'recently', 'discussed', 'ferri', 'libby', '', 'present', 'paper', '', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'small', 'viscosity', 'investigated', '', 'shown', 'problem', 'treated', 'boundarylayer', 'approximation', '', 'novel', 'feature', 'free', 'stream', 'constant', 'vorticity', '', 'discussion', 'restricted', 'twodimensional', 'incompressible', 'steady', 'flow', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['boundary', 'layer', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', '', 'boundarylayer', 'equations', 'presented', 'steady', 'incompressible', 'flow', 'pressure', 'gradient', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'plate', 'shear', 'flow', '', 'twodimensional', 'steady', 'boundarylayer', 'problem', 'flat', 'plate', 'shear', 'flow', 'incompressible', 'fluid', 'considered', '', 'solutions', 'boundarylayer', 'thickness', '', 'skin', 'friction', '', 'velocity', 'distribution', 'boundary', 'layer', 'obtained', 'karmanpohlhausen', 'technique', '', 'comparison', 'boundary', 'layer', 'uniform', 'flow', 'also', 'made', 'show', 'effect', 'vorticity', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['onedimensional', 'transient', 'heat', 'conduction', 'doublelayer', 'slab', 'subjected', 'linear', 'heat', 'input', 'small', 'time', 'internal', '', 'analytic', 'solutions', 'presented', 'transient', 'heat', 'conduction', 'composite', 'slabs', 'exposed', 'one', 'surface', 'triangular', 'heat', 'rate', '', 'type', 'heating', 'rate', 'may', 'occur', '', 'example', '', 'aerodynamic', 'heating', '']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Remove blank space tokens"
      ],
      "metadata": {
        "id": "74MvPUS4Vi2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lent = 0\n",
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  tokens_sans_blank_space = remove_blank_space_tokens(filedata)\n",
        "  newfiles[i] = tokens_sans_blank_space\n",
        "  lent += len(newfiles[i])\n",
        "print(len(newfiles))\n",
        "print(lent)\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gskynoymVgHk",
        "outputId": "d08120aa-1b1c-4023-cab2-08323c54968a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "127377\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution', 'lift', 'increase', 'due', 'slipstream', 'different', 'angles', 'attack', 'wing', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', 'results', 'intended', 'part', 'evaluation', 'basis', 'different', 'theoretical', 'treatments', 'problem', 'comparative', 'span', 'loading', 'curves', 'together', 'supporting', 'evidence', 'showed', 'substantial', 'part', 'lift', 'increment', 'produced', 'slipstream', 'due', 'destalling', 'boundarylayercontrol', 'effect', 'integrated', 'remaining', 'lift', 'increment', 'subtracting', 'destalling', 'lift', 'found', 'agree', 'well', 'potential', 'flow', 'theory', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'specific', 'configuration', 'experiment']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', 'study', 'highspeed', 'viscous', 'flow', 'past', 'twodimensional', 'body', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'nose', 'leading', 'edge', 'body', 'consequently', 'exists', 'inviscid', 'rotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', 'situation', 'arises', 'instance', 'study', 'hypersonic', 'viscous', 'flow', 'past', 'flat', 'plate', 'situation', 'somewhat', 'different', 'prandtl', 's', 'classical', 'boundarylayer', 'problem', 'prandtls', 'original', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'irrotational', 'hypersonic', 'boundarylayer', 'problem', 'inviscid', 'free', 'stream', 'must', 'considered', 'rotational', 'possible', 'effects', 'vorticity', 'recently', 'discussed', 'ferri', 'libby', 'present', 'paper', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'small', 'viscosity', 'investigated', 'shown', 'problem', 'treated', 'boundarylayer', 'approximation', 'novel', 'feature', 'free', 'stream', 'constant', 'vorticity', 'discussion', 'restricted', 'twodimensional', 'incompressible', 'steady', 'flow']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['boundary', 'layer', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'boundarylayer', 'equations', 'presented', 'steady', 'incompressible', 'flow', 'pressure', 'gradient']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'plate', 'shear', 'flow', 'twodimensional', 'steady', 'boundarylayer', 'problem', 'flat', 'plate', 'shear', 'flow', 'incompressible', 'fluid', 'considered', 'solutions', 'boundarylayer', 'thickness', 'skin', 'friction', 'velocity', 'distribution', 'boundary', 'layer', 'obtained', 'karmanpohlhausen', 'technique', 'comparison', 'boundary', 'layer', 'uniform', 'flow', 'also', 'made', 'show', 'effect', 'vorticity']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['onedimensional', 'transient', 'heat', 'conduction', 'doublelayer', 'slab', 'subjected', 'linear', 'heat', 'input', 'small', 'time', 'internal', 'analytic', 'solutions', 'presented', 'transient', 'heat', 'conduction', 'composite', 'slabs', 'exposed', 'one', 'surface', 'triangular', 'heat', 'rate', 'type', 'heating', 'rate', 'may', 'occur', 'example', 'aerodynamic', 'heating']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fileData = newfiles"
      ],
      "metadata": {
        "id": "aWPNZytWVkei"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF Matrix [25 points]"
      ],
      "metadata": {
        "id": "8T0tr-gmWyGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining helper functions"
      ],
      "metadata": {
        "id": "_SeP56npXUwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_file_dictionary(list_of_files):\n",
        "    '''\n",
        "    Paramteres:\n",
        "        list_of_files: type(string)\n",
        "    \n",
        "    returns: file_dictionary with integer key and path_of_file as value\n",
        "    '''\n",
        "    file_dictionary = {}\n",
        "    for i in range(len(list_of_files)):\n",
        "        file_dictionary[i] = list_of_files[i]\n",
        "    \n",
        "    return file_dictionary"
      ],
      "metadata": {
        "id": "1W2DO-lqWEs8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_2_way_mapping_of_term_and_doc(global_vocabulary):\n",
        "    '''\n",
        "    Parameter:\n",
        "        global_vocabulary: type(list)\n",
        "    \n",
        "    returns: 2 dictionaries with key-value pair as term:docID and docID:term respectively\n",
        "    '''\n",
        "    term_vs_ID_dict = {}\n",
        "    ID_vs_term_dict = {}\n",
        "    \n",
        "    for idx,term in enumerate(global_vocabulary):\n",
        "        ID_vs_term_dict[idx] = term\n",
        "        term_vs_ID_dict[term] = idx\n",
        "    \n",
        "    return term_vs_ID_dict, ID_vs_term_dict"
      ],
      "metadata": {
        "id": "nAMbdqemXbdF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_total_words_in_doc(TF_IDF_doc_vs_term_matrix):\n",
        "    return np.sum(TF_IDF_doc_vs_term_matrix, axis=1)"
      ],
      "metadata": {
        "id": "YSdU3wsqXgsm"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_freq_term_in_doc(matrix):\n",
        "    return np.amax(matrix, axis=1)"
      ],
      "metadata": {
        "id": "-Xl4VE0hXmfp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_IDF(inverted_index, total_documents):\n",
        "    IDF = {}\n",
        "    for term in inverted_index:\n",
        "        IDF[term] = np.log10(total_documents/(1 + len(inverted_index[term])))\n",
        "    return IDF"
      ],
      "metadata": {
        "id": "iwXsz7toXory"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Binary_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix):\n",
        "    '''\n",
        "    Parameters:\n",
        "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
        "    \n",
        "    returns: binary weighting scheme\n",
        "    '''\n",
        "    return np.where((TF_IDF_doc_vs_term_matrix <= 0),0,1).astype('float')"
      ],
      "metadata": {
        "id": "fYYomfLEXr1e"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Raw_count_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix):\n",
        "    '''\n",
        "    Parameters:\n",
        "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
        "    \n",
        "    returns: raw count weighing scheme\n",
        "    '''\n",
        "    return TF_IDF_doc_vs_term_matrix"
      ],
      "metadata": {
        "id": "WpFWL-XEXv4d"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Raw_count_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix):\n",
        "    '''\n",
        "    Parameters:\n",
        "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
        "    \n",
        "    returns: raw count weighing scheme\n",
        "    '''\n",
        "    return TF_IDF_doc_vs_term_matrix"
      ],
      "metadata": {
        "id": "gKdoYxUUX05A"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Term_frequency_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix, total_words_in_doc):\n",
        "    '''\n",
        "    Parameters:\n",
        "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
        "        total_words_in_doc: type(np.array)\n",
        "    \n",
        "    returns: term frequency weighing scheme\n",
        "    '''\n",
        "    return (TF_IDF_doc_vs_term_matrix.T / total_words_in_doc).T"
      ],
      "metadata": {
        "id": "ESpvsDNFdO2P"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Log_Normalization_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix):\n",
        "    '''\n",
        "    Parameters:\n",
        "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
        "    \n",
        "    returns: log normalization weighing scheme\n",
        "    '''\n",
        "    return np.log10(1+TF_IDF_doc_vs_term_matrix)"
      ],
      "metadata": {
        "id": "qyYwDiaQX3y3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Double_Normalization_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix, max_freq_term_in_doc):\n",
        "    '''\n",
        "    Parameters:\n",
        "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
        "    \n",
        "    returns: double normalization weighing scheme\n",
        "    '''\n",
        "    matrix = (TF_IDF_doc_vs_term_matrix.T / max_freq_term_in_doc).T\n",
        "\n",
        "    return 0.5 + 0.5*matrix"
      ],
      "metadata": {
        "id": "KgvBC9lwX61J"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_variants(matrix, total_terms, max_freq, size_of_global_vocabulary, IDF, ID_vs_term_dict, isQuery):\n",
        "    '''\n",
        "    returns: variants of TF-IDF\n",
        "    '''\n",
        "    Binary = Binary_weighing_TF_IDF(matrix)\n",
        "    Raw_count = Raw_count_weighing_TF_IDF(matrix)\n",
        "    Term_frequency = Term_frequency_weighing_TF_IDF(matrix, total_terms)\n",
        "    Log_Normalization = Log_Normalization_weighing_TF_IDF(matrix)\n",
        "    Double_Normalization = Double_Normalization_weighing_TF_IDF(matrix, max_freq)\n",
        "    \n",
        "    if(isQuery):\n",
        "        for i in tqdm(range(size_of_global_vocabulary)):\n",
        "            IDF_factor = IDF[ID_vs_term_dict[i]]\n",
        "            Binary[i] *= IDF_factor\n",
        "            Raw_count[i] *= IDF_factor\n",
        "            Term_frequency[i] *= IDF_factor\n",
        "            Log_Normalization[i] *= IDF_factor\n",
        "            Double_Normalization[i] *= IDF_factor\n",
        "    else:\n",
        "        for i in tqdm(range(size_of_global_vocabulary)):\n",
        "            IDF_factor = IDF[ID_vs_term_dict[i]]\n",
        "            Binary[:,i] *= IDF_factor\n",
        "            Raw_count[:,i] *= IDF_factor\n",
        "            Term_frequency[:,i] *= IDF_factor\n",
        "            Log_Normalization[:, i] *= IDF_factor\n",
        "            Double_Normalization[:, i] *= IDF_factor\n",
        "    \n",
        "    return Binary, Raw_count, Term_frequency, Log_Normalization, Double_Normalization"
      ],
      "metadata": {
        "id": "QzWNmUw7X9Hn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topN(N, TF_IDF, query_of_same_variant, file_dictionary):\n",
        "    '''\n",
        "    evaluates and displays top N relevant documents based on score\n",
        "    '''\n",
        "    tf_idf_score = np.dot(TF_IDF, query_of_same_variant)\n",
        "    tf_idf_score = {file_dictionary[i]:tf_idf_score[i] for i in range(len(tf_idf_score))}\n",
        "    \n",
        "    relevant_docs = list(sorted(tf_idf_score.items(), key=operator.itemgetter(1),reverse=True))[:N]\n",
        "    for docs in relevant_docs:\n",
        "        print('Score: {}  Document: {}'.format(docs[1], docs[0]))\n",
        "    print()"
      ],
      "metadata": {
        "id": "4P35L72QYHF_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAIN"
      ],
      "metadata": {
        "id": "9I4rnbRxYPB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # create set of stop words for preprocessing\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    \n",
        "    # Get List of Files in Dataset\n",
        "    list_of_files = getListOfFiles('/content/CSE508_Winter2023_Dataset/')\n",
        "    \n",
        "    # create dictionary of file with docID (integer) as key and full_path of file as value\n",
        "    file_dictionary = create_file_dictionary(list_of_files)\n",
        "    total_documents = len(file_dictionary)\n",
        "    \n",
        "    # preprocessed_list_of_docs_tokens = preprocess_documents(list_of_files, stopwords_set)\n",
        "    pi_file = open('Q1_tf_idf.pkl', 'rb')\n",
        "    preprocessed_list_of_docs_tokens = pickle.load(pi_file)\n",
        "    pi_file.close()\n",
        "    \n",
        "    # Global list of terms\n",
        "    global_list_of_terms = []\n",
        "    for doc in preprocessed_list_of_docs_tokens:\n",
        "        global_list_of_terms.extend(doc)\n",
        "    \n",
        "    # finding all distinct terms across all documents\n",
        "    global_vocabulary = list(set(global_list_of_terms))\n",
        "    size_of_global_vocabulary = len(global_vocabulary)\n",
        "    \n",
        "    # term vs docID 2 way mapping\n",
        "    term_vs_ID_dict, ID_vs_term_dict = create_2_way_mapping_of_term_and_doc(global_vocabulary)\n",
        "    \n",
        "    # term-doc matrix (for storing frequency of each word of global_vocabulary in doc)\n",
        "    inverted_index = {}\n",
        "    size_of_TF_IDF_matrix = (total_documents, size_of_global_vocabulary)\n",
        "    TF_IDF_doc_vs_term_matrix = np.zeros(size_of_TF_IDF_matrix, dtype=float)\n",
        "    \n",
        "    for i in tqdm(range(len(preprocessed_list_of_docs_tokens))):\n",
        "        for term in preprocessed_list_of_docs_tokens[i]:\n",
        "            if(term in inverted_index):\n",
        "                if(inverted_index[term][-1]!=i):\n",
        "                    inverted_index[term].append(i)\n",
        "            else:\n",
        "                inverted_index[term] = [i]\n",
        "            TF_IDF_doc_vs_term_matrix[i][term_vs_ID_dict[term]] += 1\n",
        "    \n",
        "    total_words_in_doc = get_total_words_in_doc(TF_IDF_doc_vs_term_matrix)\n",
        "    \n",
        "    max_freq_term_in_doc = get_max_freq_term_in_doc(TF_IDF_doc_vs_term_matrix)\n",
        "    \n",
        "    # Calculate IDF\n",
        "    IDF = calculate_IDF(inverted_index, total_documents)\n",
        "    \n",
        "    Binary_tf_idf, Raw_count_tf_idf, Term_frequency_tf_idf, Log_Normalization_tf_idf, Double_Normalization_tf_idf = compute_variants(TF_IDF_doc_vs_term_matrix, total_words_in_doc, max_freq_term_in_doc, size_of_global_vocabulary, IDF, ID_vs_term_dict, False)\n",
        "    \n",
        "    query = input(\"Input query: \")\n",
        "    sanitized_query = preprocess(query, stopwords_set)\n",
        "    print(\"Sanitized query: \", sanitized_query)\n",
        "\n",
        "    query_frequency = {}\n",
        "    for query_token in sanitized_query:\n",
        "        if query_token in query_frequency:\n",
        "            query_frequency[query_token]+=1\n",
        "        else:\n",
        "            query_frequency[query_token]=1\n",
        "    \n",
        "    # create query frequency vector\n",
        "    query_frequency_vector = np.zeros((size_of_global_vocabulary,1))\n",
        "    for token in query_frequency.keys():\n",
        "        if token in global_vocabulary:\n",
        "            query_frequency_vector[term_vs_ID_dict[token]] = query_frequency[token]\n",
        "    \n",
        "    \n",
        "    max_freq_token = query_frequency[max(query_frequency, key=query_frequency.get)]\n",
        "    query_binary, query_raw_count, query_term_frequency, query_log_normalization, query_double_normalization = compute_variants(query_frequency_vector, len(sanitized_query), max_freq_token, size_of_global_vocabulary, IDF, ID_vs_term_dict, True)\n",
        "    \n",
        "    print('Binary Scheme: Top 5 relevant documents are:')\n",
        "    topN(5, Binary_tf_idf, query_binary, file_dictionary)\n",
        "    \n",
        "    print('Raw Count Scheme: Top 5 relevant documents are:')\n",
        "    topN(5, Raw_count_tf_idf, query_raw_count, file_dictionary)\n",
        "    \n",
        "    print('Term Frequency Scheme: Top 5 relevant documents are:')\n",
        "    topN(5, Term_frequency_tf_idf, query_term_frequency, file_dictionary)\n",
        "    \n",
        "    print('Log Normalization Scheme: Top 5 relevant documents are:')\n",
        "    topN(5, Log_Normalization_tf_idf, query_log_normalization, file_dictionary)\n",
        "    \n",
        "    print('Double_Normalization Scheme: Top 5 relevant documents are:')\n",
        "    topN(5, Double_Normalization_tf_idf, query_double_normalization, file_dictionary)"
      ],
      "metadata": {
        "id": "MG0utMO-YLbG"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVPbv2AMYbh3",
        "outputId": "d6051984-8a14-4ec2-bf32-fa23685bd178"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1400/1400 [00:00<00:00, 7953.71it/s]\n",
            "100%|██████████| 8996/8996 [00:00<00:00, 13718.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input query: study\n",
            "Sanitized query:  ['study']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8996/8996 [00:00<00:00, 78218.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Scheme: Top 5 relevant documents are:\n",
            "Score: [0.9938274]  Document: /content/CSE508_Winter2023_Dataset/cranfield0001\n",
            "Score: [0.9938274]  Document: /content/CSE508_Winter2023_Dataset/cranfield0002\n",
            "Score: [0.9938274]  Document: /content/CSE508_Winter2023_Dataset/cranfield0008\n",
            "Score: [0.9938274]  Document: /content/CSE508_Winter2023_Dataset/cranfield0011\n",
            "Score: [0.9938274]  Document: /content/CSE508_Winter2023_Dataset/cranfield0017\n",
            "\n",
            "Raw Count Scheme: Top 5 relevant documents are:\n",
            "Score: [2.9814822]  Document: /content/CSE508_Winter2023_Dataset/cranfield0421\n",
            "Score: [2.9814822]  Document: /content/CSE508_Winter2023_Dataset/cranfield0907\n",
            "Score: [2.9814822]  Document: /content/CSE508_Winter2023_Dataset/cranfield0991\n",
            "Score: [1.9876548]  Document: /content/CSE508_Winter2023_Dataset/cranfield0002\n",
            "Score: [1.9876548]  Document: /content/CSE508_Winter2023_Dataset/cranfield0056\n",
            "\n",
            "Term Frequency Scheme: Top 5 relevant documents are:\n",
            "Score: [0.06211421]  Document: /content/CSE508_Winter2023_Dataset/cranfield0507\n",
            "Score: [0.03897362]  Document: /content/CSE508_Winter2023_Dataset/cranfield0716\n",
            "Score: [0.03487114]  Document: /content/CSE508_Winter2023_Dataset/cranfield0941\n",
            "Score: [0.03105711]  Document: /content/CSE508_Winter2023_Dataset/cranfield0862\n",
            "Score: [0.03011598]  Document: /content/CSE508_Winter2023_Dataset/cranfield0421\n",
            "\n",
            "Log Normalization Scheme: Top 5 relevant documents are:\n",
            "Score: [0.18011941]  Document: /content/CSE508_Winter2023_Dataset/cranfield0421\n",
            "Score: [0.18011941]  Document: /content/CSE508_Winter2023_Dataset/cranfield0907\n",
            "Score: [0.18011941]  Document: /content/CSE508_Winter2023_Dataset/cranfield0991\n",
            "Score: [0.14274125]  Document: /content/CSE508_Winter2023_Dataset/cranfield0002\n",
            "Score: [0.14274125]  Document: /content/CSE508_Winter2023_Dataset/cranfield0056\n",
            "\n",
            "Double_Normalization Scheme: Top 5 relevant documents are:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: [14671.00742777]  Document: /content/CSE508_Winter2023_Dataset/cranfield0718\n",
            "Score: [14658.84278181]  Document: /content/CSE508_Winter2023_Dataset/cranfield0212\n",
            "Score: [14654.71266031]  Document: /content/CSE508_Winter2023_Dataset/cranfield1167\n",
            "Score: [14654.42523678]  Document: /content/CSE508_Winter2023_Dataset/cranfield0344\n",
            "Score: [14653.87756631]  Document: /content/CSE508_Winter2023_Dataset/cranfield1088\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Jaccard Coefficient [15 marks]"
      ],
      "metadata": {
        "id": "EaWz9MARe-EU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Union function"
      ],
      "metadata": {
        "id": "TiQAD1jlgLCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def union(l1,l2):\n",
        "    union_list = list(set(l1) | set(l2))\n",
        "    return union_list"
      ],
      "metadata": {
        "id": "HfM1ymDGcl5A"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intersection function"
      ],
      "metadata": {
        "id": "sO6JvPMmgScW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection(l1,l2):\n",
        "    intersection_list = list(set(l1) & set(l2))\n",
        "    return intersection_list"
      ],
      "metadata": {
        "id": "0LL4cP_PgNF9"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Jaccard coefficient"
      ],
      "metadata": {
        "id": "d5h5MHaMgZSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccardCoefficient(query, list_of_files):\n",
        "    for i,filePath in enumerate (list_of_files):\n",
        "        file = open(filePath, encoding=\"utf8\", errors = \"ignore\")\n",
        "        read = file.read()    \n",
        "        file.close()\n",
        "    \n",
        "        sanitized_query = preprocess(read, stopwords_set)                    \n",
        "        # calculate jaccard coefficient value based on formula that is intersection of document and query divided by union of \n",
        "        # document and query\n",
        "        jaccard_coefficient[filePath] = len(intersection(sanitized_query, query))/len(union(sanitized_query, query))"
      ],
      "metadata": {
        "id": "LJrNuDt9gYNy"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJaAWIuXhzzQ",
        "outputId": "24b261d3-2567-4a58-d4c5-43dab3f3b6d6"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding top 5 relevant documents"
      ],
      "metadata": {
        "id": "t2zs2m4ogjtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get List of Files in Dataset\n",
        "list_of_files = getListOfFiles(\"/content/CSE508_Winter2023_Dataset/\")\n",
        "#print(list_of_files)\n",
        "jaccard_coefficient = {}        \n",
        "sentence_query = input(\"Enter the query: \")\n",
        "query = preprocess(sentence_query, stopwords_set)      # Query Processing\n",
        "jaccardCoefficient(query, list_of_files)\n",
        "# it counts the elements in the dictionary and prints the 5 most common documents based on it.\n",
        "#k = Counter(dict(jaccard_coefficient)).most_common(5) \n",
        "print(\" Top 5 relevant documents based on Jaccard Coefficient \")  \n",
        "for i in Counter(dict(jaccard_coefficient)).most_common(10) :\n",
        "    print(\"{} --> {}\".format(i[1],i[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5HuOLKugWKM",
        "outputId": "ff481576-0697-4ada-81e2-0103476dfbde"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the query: supersonic wind tunnel\n",
            " Top 5 relevant documents based on Jaccard Coefficient \n",
            "0.10714285714285714 --> /content/CSE508_Winter2023_Dataset/cranfield0430\n",
            "0.08695652173913043 --> /content/CSE508_Winter2023_Dataset/cranfield0429\n",
            "0.07692307692307693 --> /content/CSE508_Winter2023_Dataset/cranfield1142\n",
            "0.07407407407407407 --> /content/CSE508_Winter2023_Dataset/cranfield1306\n",
            "0.06976744186046512 --> /content/CSE508_Winter2023_Dataset/cranfield0969\n",
            "0.06896551724137931 --> /content/CSE508_Winter2023_Dataset/cranfield0594\n",
            "0.06666666666666667 --> /content/CSE508_Winter2023_Dataset/cranfield0598\n",
            "0.06666666666666667 --> /content/CSE508_Winter2023_Dataset/cranfield0795\n",
            "0.06666666666666667 --> /content/CSE508_Winter2023_Dataset/cranfield1243\n",
            "0.06060606060606061 --> /content/CSE508_Winter2023_Dataset/cranfield1305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# queries -> jet propulsion, supersonic wind tunnel"
      ],
      "metadata": {
        "id": "UqDVck2NiBP0"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I-E8J4bHkXgz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}